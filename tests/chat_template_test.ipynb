{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8a54f0f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "import requests\n",
    "from datasets import load_from_disk\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "ea4b720c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "96ef7ff431cd4ca9baa628d45a3d3a64",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/8 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"/root/group-shared/jrc/base_models/Qwen3-14B\")\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    \"/root/group-shared/jrc/base_models/Qwen3-14B\",\n",
    "    device_map=\"auto\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6107125f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "num_proc must be <= 1. Reducing num_proc to 1 for dataset of size 1.\n",
      "WARNING:datasets.arrow_dataset:num_proc must be <= 1. Reducing num_proc to 1 for dataset of size 1.\n",
      "num_proc must be <= 1. Reducing num_proc to 1 for dataset of size 1.\n",
      "WARNING:datasets.arrow_dataset:num_proc must be <= 1. Reducing num_proc to 1 for dataset of size 1.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'instruction': 'Provide a step-by-step solution to the hard math problem, with the final answer boxed at the end.', 'dataset_name': 'aime_2024', 'prompt': '<|im_start|>system\\nYou are a helpful assistant.<|im_end|>\\n<|im_start|>user\\nRewrite the following instruction via rephrasing and/or adding specific requirements. Add instructions which would be helpful to solve the problem correctly. Output the new instruction only.\\nInstruction: Provide a step-by-step solution to the hard math problem, with the final answer boxed at the end.<|im_end|>\\n<|im_start|>assistant\\n<think>\\n\\n</think>\\n\\n'}\n"
     ]
    }
   ],
   "source": [
    "dataset = load_from_disk(\"/root/workspace/PRewrite/data/aime_2024_only\")\n",
    "meta_instruction = \"Rewrite the following instruction via rephrasing and/or adding specific requirements. Add instructions which would be helpful to solve the problem correctly. Output the new instruction only.\"\n",
    "\n",
    "def add_meta_instruction(element):\n",
    "    tokenizer = AutoTokenizer.from_pretrained(\"/root/group-shared/jrc/base_models/Qwen3-14B\")\n",
    "    element[\"prompt\"] = tokenizer.apply_chat_template(\n",
    "        [\n",
    "            {\n",
    "                \"role\": \"system\",\n",
    "                \"content\": \"You are a helpful assistant.\"\n",
    "            },\n",
    "            {\n",
    "                \"role\": \"user\",\n",
    "                \"content\": f\"{meta_instruction}\\nInstruction: {element['instruction']}\"\n",
    "            }\n",
    "        ],\n",
    "        tokenize=False,\n",
    "        add_generation_prompt=True,\n",
    "        enable_thinking=False,\n",
    "    )\n",
    "    return element\n",
    "\n",
    "dataset = dataset.map(add_meta_instruction, num_proc = 8, desc = \"Adding meta instruction\")\n",
    "\n",
    "print(dataset[\"train\"][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "26aef1a7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1d212fc0fc84414c87185e98a0c8c9dd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Changing initial instruction:   0%|          | 0/1 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following generation flags are not valid and may be ignored: ['temperature', 'top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "45b3e998b5a94194b12ff0d3fbfff8e2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Changing initial instruction:   0%|          | 0/1 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'instruction': 'Provide a clear, step-by-step solution to the challenging math problem, explaining each part of the process in detail. Ensure that all mathematical operations are shown correctly, and include any necessary formulas or reasoning. Make sure the solution is complete and leads logically to the final answer, which should be clearly presented in a box at the end.', 'dataset_name': 'aime_2024', 'prompt': '<|im_start|>system\\nYou are a helpful assistant.<|im_end|>\\n<|im_start|>user\\nRewrite the following instruction via rephrasing and/or adding specific requirements. Add instructions which would be helpful to solve the problem correctly. Output the new instruction only.\\nInstruction: Provide a clear, step-by-step solution to the challenging math problem, explaining each part of the process in detail. Ensure that all mathematical operations are shown correctly, and include any necessary formulas or reasoning. Make sure the solution is complete and leads logically to the final answer, which should be clearly presented in a box at the end.<|im_end|>\\n<|im_start|>assistant\\n<think>\\n\\n</think>\\n\\n'}\n"
     ]
    }
   ],
   "source": [
    "def change_initial_instruction(element):\n",
    "    tokenizer = AutoTokenizer.from_pretrained(\"/root/group-shared/jrc/base_models/Qwen3-14B\")\n",
    "    inputs = tokenizer(\n",
    "        [element[\"prompt\"]],\n",
    "        return_tensors=\"pt\"\n",
    "    )\n",
    "    \n",
    "    inputs.to(model.device)\n",
    "\n",
    "    output_ids = model.generate(\n",
    "        inputs[\"input_ids\"],\n",
    "        attention_mask=inputs[\"attention_mask\"],\n",
    "        max_new_tokens=1024,\n",
    "        do_sample=False\n",
    "    )\n",
    "    element[\"instruction\"] = tokenizer.decode(output_ids[0], skip_special_tokens=True).split(\"</think>\")[-1].strip()\n",
    "    element[\"prompt\"] = tokenizer.apply_chat_template(\n",
    "        [\n",
    "            {\n",
    "                \"role\": \"system\",\n",
    "                \"content\": \"You are a helpful assistant.\"\n",
    "            },\n",
    "            {\n",
    "                \"role\": \"user\",\n",
    "                \"content\": f\"{meta_instruction}\\nInstruction: {element['instruction']}\"\n",
    "            }\n",
    "        ],\n",
    "        tokenize=False,\n",
    "        add_generation_prompt=True,\n",
    "        enable_thinking=False,\n",
    "    )\n",
    "    return element\n",
    "\n",
    "dataset = dataset.map(change_initial_instruction, desc = \"Changing initial instruction\")\n",
    "\n",
    "print(dataset[\"train\"][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e9d7ee92",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<|im_start|>system\n",
      "You are a helpful assistant.<|im_end|>\n",
      "<|im_start|>user\n",
      "Rewrite the following via rephrasing and/or adding specific requirements. According to the subject of the problem, add particular instructions which would be helpful to solve the problem correctly. Output the new instruction only.\n",
      "Instruction: Solution to the math problem in subject **Number Theory**.<|im_end|>\n",
      "<|im_start|>assistant\n",
      "<think>\n",
      "\n",
      "</think>\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "meta_prompt = \"\"\"Rewrite the following via rephrasing and/or adding specific requirements. According to the subject of the problem, add particular instructions which would be helpful to solve the problem correctly. Output the new instruction only.\n",
    "Instruction: Solution to the math problem in subject **Number Theory**.\"\"\"\n",
    "\n",
    "prompt = tokenizer.apply_chat_template(\n",
    "    [\n",
    "        {\n",
    "            \"role\": \"system\",\n",
    "            \"content\": \"You are a helpful assistant.\"\n",
    "        },\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": meta_prompt\n",
    "        }\n",
    "    ],\n",
    "    tokenize=False,\n",
    "    add_generation_prompt=True,\n",
    "    enable_thinking=False,\n",
    ")\n",
    "print(prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "adf1c703",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Provide a detailed solution to the math problem in the subject **Number Theory**, including step-by-step reasoning. Use appropriate theorems, properties, or lemmas relevant to number theory (such as divisibility rules, modular arithmetic, prime factorization, or the fundamental theorem of arithmetic) as needed. Ensure clarity in your logic and explicitly show how each step contributes to the final solution. If the problem involves proof, write the proof in a formal and structured manner.\n"
     ]
    }
   ],
   "source": [
    "generate_model_config = {\n",
    "    \"temperature\": 1,\n",
    "    \"max_new_tokens\": 4096,\n",
    "}\n",
    "request = requests.post(\n",
    "    \"http://0.0.0.0:33337/generate\",\n",
    "    json={\"text\": prompt, \"sampling_params\": generate_model_config},\n",
    ")\n",
    "rewrite_prompt = request.json()['text']\n",
    "print(rewrite_prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "c0f90776",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# Instruction\n",
      "Provide a detailed and step-by-step solution to the challenging math problem: {problem}  \n",
      "The subject of the problem is {subject}.  \n",
      "The level of difficulty is {difficulty}.  \n",
      "Ensure your solution includes clear explanations for each step, any relevant formulas or theorems used, and, where applicable, a logical breakdown of the problem-solving process. Verify your final answer to ensure correctness.\n",
      "False\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "print(rewrite_prompt)\n",
    "tags = [\"{problem}\", \"{subject}\", \"{difficulty}\",\"{hello}\"]\n",
    "print(set(re.findall(r\"{[^}]+}\", rewrite_prompt)) == set(tags))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "153b3c71",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# Instruction\n",
      "Provide a detailed and step-by-step solution to the challenging math problem: Wendy is five times as old as Colin will be seven years from now. In 25 years, Colin will be a third as old as Wendy is now. Compute Colin's current age.  \n",
      "The subject of the problem is math.  \n",
      "The level of difficulty is 1.  \n",
      "Ensure your solution includes clear explanations for each step, any relevant formulas or theorems used, and, where applicable, a logical breakdown of the problem-solving process. Verify your final answer to ensure correctness.\n"
     ]
    }
   ],
   "source": [
    "data = {\n",
    "    \"problem\": \"Wendy is five times as old as Colin will be seven years from now. In 25 years, Colin will be a third as old as Wendy is now. Compute Colin's current age.\",\n",
    "    \"subject\": \"math\",\n",
    "    \"difficulty\": 1,\n",
    "    \"hello\": \"world\"\n",
    "}\n",
    "print(rewrite_prompt.format(**data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42c2daa9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# æ–¹æ³•1: ä½¿ç”¨æ¸…ç†å‡½æ•°ï¼ˆæ¨èï¼‰\n",
    "# æ³¨æ„ï¼šå¦‚æœclear_gpu_memoryå‡½æ•°è¿˜æ²¡å®šä¹‰ï¼Œå…ˆè¿è¡ŒCell 10\n",
    "try:\n",
    "    clear_gpu_memory(\n",
    "        model=model,\n",
    "        functions_to_clear=['change_initial_instruction']  # åˆ é™¤å¯èƒ½æŒæœ‰æ¨¡å‹å¼•ç”¨çš„å‡½æ•°\n",
    "    )\n",
    "except NameError:\n",
    "    # å¦‚æœå‡½æ•°è¿˜æ²¡å®šä¹‰ï¼Œä½¿ç”¨æ‰‹åŠ¨æ¸…ç†\n",
    "    print(\"âš ï¸ clear_gpu_memoryå‡½æ•°æœªå®šä¹‰ï¼Œä½¿ç”¨æ‰‹åŠ¨æ¸…ç†...\")\n",
    "    import torch\n",
    "    import gc\n",
    "    \n",
    "    # åˆ é™¤å‡½æ•°ä¸­çš„é—­åŒ…å¼•ç”¨\n",
    "    if 'change_initial_instruction' in globals():\n",
    "        del change_initial_instruction\n",
    "    \n",
    "    # åˆ é™¤æ¨¡å‹\n",
    "    if 'model' in globals():\n",
    "        del model\n",
    "    \n",
    "    # æ¸…ç†GPU\n",
    "    if torch.cuda.is_available():\n",
    "        for i in range(torch.cuda.device_count()):\n",
    "            with torch.cuda.device(i):\n",
    "                torch.cuda.empty_cache()\n",
    "        torch.cuda.synchronize()\n",
    "    \n",
    "    gc.collect()\n",
    "    print(\"âœ… æ‰‹åŠ¨æ¸…ç†å®Œæˆ\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15cb2a25",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "15604"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# å½»åº•æ¸…ç†æ˜¾å­˜çš„æ–¹æ³•\n",
    "import torch\n",
    "import gc\n",
    "\n",
    "# æ–¹æ³•1: åˆ é™¤æ‰€æœ‰å¯¹æ¨¡å‹çš„å¼•ç”¨\n",
    "# æ³¨æ„ï¼šåœ¨notebookä¸­ï¼Œéœ€è¦ç¡®ä¿æ²¡æœ‰å…¶ä»–å˜é‡å¼•ç”¨æ¨¡å‹\n",
    "try:\n",
    "    del model\n",
    "except NameError:\n",
    "    pass\n",
    "\n",
    "# æ–¹æ³•2: æ¸…ç†æ‰€æœ‰GPUç¼“å­˜\n",
    "if torch.cuda.is_available():\n",
    "    # æ¸…ç†æ‰€æœ‰GPUè®¾å¤‡\n",
    "    for i in range(torch.cuda.device_count()):\n",
    "        with torch.cuda.device(i):\n",
    "            torch.cuda.empty_cache()\n",
    "            torch.cuda.ipc_collect()  # æ¸…ç†è¿›ç¨‹é—´é€šä¿¡ç¼“å­˜\n",
    "    \n",
    "    # åŒæ­¥æ‰€æœ‰è®¾å¤‡\n",
    "    torch.cuda.synchronize()\n",
    "\n",
    "# æ–¹æ³•3: Pythonåƒåœ¾å›æ”¶\n",
    "gc.collect()\n",
    "\n",
    "# æ–¹æ³•4: å¼ºåˆ¶æ¸…ç†ï¼ˆå¦‚æœä½¿ç”¨accelerateçš„device_mapï¼‰\n",
    "try:\n",
    "    import accelerate\n",
    "    # å¦‚æœæ¨¡å‹ä½¿ç”¨äº†accelerateçš„è®¾å¤‡æ˜ å°„ï¼Œéœ€è¦é¢å¤–æ¸…ç†\n",
    "    pass\n",
    "except ImportError:\n",
    "    pass\n",
    "\n",
    "# æ£€æŸ¥æ˜¾å­˜ä½¿ç”¨æƒ…å†µ\n",
    "if torch.cuda.is_available():\n",
    "    print(\"GPUæ˜¾å­˜ä½¿ç”¨æƒ…å†µ:\")\n",
    "    for i in range(torch.cuda.device_count()):\n",
    "        allocated = torch.cuda.memory_allocated(i) / 1024**3  # GB\n",
    "        reserved = torch.cuda.memory_reserved(i) / 1024**3  # GB\n",
    "        print(f\"  GPU {i}: å·²åˆ†é… {allocated:.2f} GB, å·²ä¿ç•™ {reserved:.2f} GB\")\n",
    "else:\n",
    "    print(\"CUDAä¸å¯ç”¨\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f16e76d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# é€šç”¨çš„æ˜¾å­˜æ¸…ç†å‡½æ•°ï¼ˆæ¨èä½¿ç”¨ï¼‰\n",
    "def clear_gpu_memory(model=None, tokenizer=None, functions_to_clear=None):\n",
    "    \"\"\"\n",
    "    å½»åº•æ¸…ç†GPUæ˜¾å­˜\n",
    "    \n",
    "    Args:\n",
    "        model: è¦åˆ é™¤çš„æ¨¡å‹å¯¹è±¡\n",
    "        tokenizer: è¦åˆ é™¤çš„tokenizerå¯¹è±¡ï¼ˆå¯é€‰ï¼‰\n",
    "        functions_to_clear: è¦åˆ é™¤çš„å‡½æ•°åˆ—è¡¨ï¼Œè¿™äº›å‡½æ•°å¯èƒ½æŒæœ‰æ¨¡å‹çš„é—­åŒ…å¼•ç”¨\n",
    "    \"\"\"\n",
    "    import torch\n",
    "    import gc\n",
    "    \n",
    "    # 1. åˆ é™¤å¯èƒ½æŒæœ‰æ¨¡å‹å¼•ç”¨çš„å‡½æ•°\n",
    "    if functions_to_clear:\n",
    "        for func_name in functions_to_clear:\n",
    "            if func_name in globals():\n",
    "                del globals()[func_name]\n",
    "                print(f\"âœ… å·²åˆ é™¤å‡½æ•°: {func_name}\")\n",
    "    \n",
    "    # 2. åˆ é™¤æ¨¡å‹\n",
    "    if model is not None:\n",
    "        # å¦‚æœæ¨¡å‹ä½¿ç”¨äº†device_mapï¼Œéœ€è¦ç‰¹æ®Šå¤„ç†\n",
    "        if hasattr(model, 'hf_device_map') and model.hf_device_map:\n",
    "            # å¯¹äºä½¿ç”¨device_mapçš„æ¨¡å‹ï¼Œéœ€è¦æ‰‹åŠ¨æ¸…ç†æ¯ä¸ªè®¾å¤‡\n",
    "            for device in model.hf_device_map.values():\n",
    "                if isinstance(device, str) and 'cuda' in device:\n",
    "                    device_id = int(device.split(':')[-1]) if ':' in device else 0\n",
    "                    with torch.cuda.device(device_id):\n",
    "                        torch.cuda.empty_cache()\n",
    "        \n",
    "        # åˆ é™¤æ¨¡å‹å¯¹è±¡\n",
    "        del model\n",
    "        print(\"âœ… æ¨¡å‹å·²åˆ é™¤\")\n",
    "    \n",
    "    # 3. åˆ é™¤tokenizerï¼ˆå¯é€‰ï¼‰\n",
    "    if tokenizer is not None:\n",
    "        del tokenizer\n",
    "        print(\"âœ… Tokenizerå·²åˆ é™¤\")\n",
    "    \n",
    "    # 4. æ¸…ç†æ‰€æœ‰GPUç¼“å­˜\n",
    "    if torch.cuda.is_available():\n",
    "        for i in range(torch.cuda.device_count()):\n",
    "            with torch.cuda.device(i):\n",
    "                torch.cuda.empty_cache()\n",
    "                torch.cuda.ipc_collect()\n",
    "        torch.cuda.synchronize()\n",
    "        print(\"âœ… GPUç¼“å­˜å·²æ¸…ç†\")\n",
    "    \n",
    "    # 5. Pythonåƒåœ¾å›æ”¶\n",
    "    gc.collect()\n",
    "    print(\"âœ… Pythonåƒåœ¾å›æ”¶å®Œæˆ\")\n",
    "    \n",
    "    # 6. æ˜¾ç¤ºæ˜¾å­˜ä½¿ç”¨æƒ…å†µ\n",
    "    if torch.cuda.is_available():\n",
    "        print(\"\\nğŸ“Š å½“å‰GPUæ˜¾å­˜ä½¿ç”¨æƒ…å†µ:\")\n",
    "        for i in range(torch.cuda.device_count()):\n",
    "            allocated = torch.cuda.memory_allocated(i) / 1024**3\n",
    "            reserved = torch.cuda.memory_reserved(i) / 1024**3\n",
    "            print(f\"  GPU {i}: å·²åˆ†é… {allocated:.2f} GB, å·²ä¿ç•™ {reserved:.2f} GB\")\n",
    "\n",
    "# ä½¿ç”¨ç¤ºä¾‹ï¼š\n",
    "# clear_gpu_memory(\n",
    "#     model=model,\n",
    "#     tokenizer=tokenizer,  # å¯é€‰\n",
    "#     functions_to_clear=['change_initial_instruction']  # å¯é€‰\n",
    "# )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "c618aa8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "del model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "160b7dbd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… GPUç¼“å­˜å·²æ¸…ç†\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "import gc\n",
    "if torch.cuda.is_available():\n",
    "    for i in range(torch.cuda.device_count()):\n",
    "        with torch.cuda.device(i):\n",
    "            torch.cuda.empty_cache()\n",
    "            torch.cuda.ipc_collect()\n",
    "    torch.cuda.synchronize()\n",
    "    print(\"âœ… GPUç¼“å­˜å·²æ¸…ç†\")\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "63cadc32",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9aaf65f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "del trainer.model\n",
    "if hasattr(trainer, 'ref_model') and trainer.ref_model is not None:\n",
    "    del trainer.ref_model\n",
    "del trainer\n",
    "\n",
    "# æ¸…ç©ºæ˜¾å­˜\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.empty_cache()\n",
    "gc.collect()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PRewrite",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
