{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e2672e94",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sglang as sgl\n",
    "import sglang.test.doc_patch\n",
    "from sglang.utils import async_stream_and_merge, stream_and_merge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ab909831",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 11-03 02:33:15 [__init__.py:216] Automatically detected platform cuda.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`torch_dtype` is deprecated! Use `dtype` instead!\n",
      "[2025-11-03 02:33:17] Using default HuggingFace chat template with detected content format: string\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 11-03 02:33:23 [__init__.py:216] Automatically detected platform cuda.\n",
      "INFO 11-03 02:33:23 [__init__.py:216] Automatically detected platform cuda.\n",
      "INFO 11-03 02:33:24 [__init__.py:216] Automatically detected platform cuda.\n",
      "INFO 11-03 02:33:24 [__init__.py:216] Automatically detected platform cuda.\n",
      "INFO 11-03 02:33:24 [__init__.py:216] Automatically detected platform cuda.\n",
      "INFO 11-03 02:33:24 [__init__.py:216] Automatically detected platform cuda.\n",
      "INFO 11-03 02:33:24 [__init__.py:216] Automatically detected platform cuda.\n",
      "INFO 11-03 02:33:24 [__init__.py:216] Automatically detected platform cuda.\n",
      "INFO 11-03 02:33:25 [__init__.py:216] Automatically detected platform cuda.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`torch_dtype` is deprecated! Use `dtype` instead!\n",
      "`torch_dtype` is deprecated! Use `dtype` instead!\n",
      "[2025-11-03 02:33:26 TP1] Init torch distributed begin.\n",
      "`torch_dtype` is deprecated! Use `dtype` instead!\n",
      "`torch_dtype` is deprecated! Use `dtype` instead!\n",
      "`torch_dtype` is deprecated! Use `dtype` instead!\n",
      "[2025-11-03 02:33:26 TP2] Init torch distributed begin.\n",
      "`torch_dtype` is deprecated! Use `dtype` instead!\n",
      "`torch_dtype` is deprecated! Use `dtype` instead!\n",
      "`torch_dtype` is deprecated! Use `dtype` instead!\n",
      "[2025-11-03 02:33:26 TP5] Init torch distributed begin.\n",
      "[2025-11-03 02:33:26 TP0] Init torch distributed begin.\n",
      "[2025-11-03 02:33:26 TP3] Init torch distributed begin.\n",
      "[2025-11-03 02:33:26 TP7] Init torch distributed begin.\n",
      "[2025-11-03 02:33:26 TP4] Init torch distributed begin.\n",
      "[2025-11-03 02:33:26 TP6] Init torch distributed begin.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Gloo] Rank 0 is connected to 7 peer ranks. Expected number of connected peer ranks is : 7\n",
      "[Gloo] Rank 1 is connected to 7 peer ranks. Expected number of connected peer ranks is : 7\n",
      "[Gloo] Rank 3 is connected to 7 peer ranks. Expected number of connected peer ranks is : 7\n",
      "[Gloo] Rank 2 is connected to 7 peer ranks. Expected number of connected peer ranks is : 7\n",
      "[Gloo] Rank 4 is connected to 7 peer ranks. Expected number of connected peer ranks is : 7\n",
      "[Gloo] Rank 5 is connected to 7 peer ranks. Expected number of connected peer ranks is : 7\n",
      "[Gloo] Rank [Gloo] Rank 7 is connected to 67 is connected to  peer ranks. 7Expected number of connected peer ranks is :  peer ranks. 7Expected number of connected peer ranks is : 7\n",
      "\n",
      "[Gloo] Rank 0 is connected to 7 peer ranks. Expected number of connected peer ranks is : 7\n",
      "[Gloo] Rank 1 is connected to 7 peer ranks. Expected number of connected peer ranks is : 7\n",
      "[Gloo] Rank 2 is connected to 7 peer ranks. Expected number of connected peer ranks is : 7\n",
      "[Gloo] Rank 3 is connected to 7 peer ranks. Expected number of connected peer ranks is : 7\n",
      "[Gloo] Rank 4 is connected to 7 peer ranks. Expected number of connected peer ranks is : 7\n",
      "[Gloo] Rank 6 is connected to 7 peer ranks. Expected number of connected peer ranks is : 7[Gloo] Rank \n",
      "5 is connected to 7 peer ranks. Expected number of connected peer ranks is : 7\n",
      "[Gloo] Rank 7 is connected to 7 peer ranks. Expected number of connected peer ranks is : 7\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2025-11-03 02:33:27 TP0] sglang is using nccl==2.27.3\n",
      "[2025-11-03 02:33:28 TP0] reading GPU P2P access cache from /root/.cache/sglang/gpu_p2p_access_cache_for_0,1,2,3,4,5,6,7.json\n",
      "[2025-11-03 02:33:28 TP3] reading GPU P2P access cache from /root/.cache/sglang/gpu_p2p_access_cache_for_0,1,2,3,4,5,6,7.json\n",
      "[2025-11-03 02:33:28 TP7] reading GPU P2P access cache from /root/.cache/sglang/gpu_p2p_access_cache_for_0,1,2,3,4,5,6,7.json\n",
      "[2025-11-03 02:33:28 TP4] reading GPU P2P access cache from /root/.cache/sglang/gpu_p2p_access_cache_for_0,1,2,3,4,5,6,7.json\n",
      "[2025-11-03 02:33:28 TP5] reading GPU P2P access cache from /root/.cache/sglang/gpu_p2p_access_cache_for_0,1,2,3,4,5,6,7.json\n",
      "[2025-11-03 02:33:28 TP1] reading GPU P2P access cache from /root/.cache/sglang/gpu_p2p_access_cache_for_0,1,2,3,4,5,6,7.json\n",
      "[2025-11-03 02:33:28 TP2] reading GPU P2P access cache from /root/.cache/sglang/gpu_p2p_access_cache_for_0,1,2,3,4,5,6,7.json\n",
      "[2025-11-03 02:33:28 TP6] reading GPU P2P access cache from /root/.cache/sglang/gpu_p2p_access_cache_for_0,1,2,3,4,5,6,7.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0\n",
      "[Gloo] Rank [Gloo] Rank 00 is connected to  is connected to 00 peer ranks.  peer ranks. Expected number of connected peer ranks is : 0Expected number of connected peer ranks is : 0\n",
      "\n",
      "[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0\n",
      "[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0\n",
      "[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0[Gloo] Rank \n",
      "0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0\n",
      "[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0\n",
      "[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0\n",
      "[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0\n",
      "[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0\n",
      "[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0\n",
      "[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0\n",
      "[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0\n",
      "[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0\n",
      "[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0\n",
      "[Gloo] Rank 0 is connected to 7 peer ranks. Expected number of connected peer ranks is : 7\n",
      "[Gloo] Rank 3 is connected to 7 peer ranks. Expected number of connected peer ranks is : 7\n",
      "[Gloo] Rank 2 is connected to 7 peer ranks. Expected number of connected peer ranks is : 7\n",
      "[Gloo] Rank 4 is connected to 7 peer ranks. Expected number of connected peer ranks is : 7\n",
      "[Gloo] Rank 1 is connected to 7 peer ranks. Expected number of connected peer ranks is : 7\n",
      "[Gloo] Rank 6 is connected to 7 peer ranks. Expected number of connected peer ranks is : 7\n",
      "[Gloo] Rank 5 is connected to 7 peer ranks. [Gloo] Rank Expected number of connected peer ranks is : 77 is connected to \n",
      "7 peer ranks. Expected number of connected peer ranks is : 7\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2025-11-03 02:33:28 TP7] Init torch distributed ends. mem usage=0.63 GB\n",
      "[2025-11-03 02:33:28 TP0] Init torch distributed ends. mem usage=0.63 GB\n",
      "[2025-11-03 02:33:28 TP6] Init torch distributed ends. mem usage=0.73 GB\n",
      "[2025-11-03 02:33:28 TP5] Init torch distributed ends. mem usage=0.73 GB\n",
      "[2025-11-03 02:33:28 TP4] Init torch distributed ends. mem usage=0.73 GB\n",
      "[2025-11-03 02:33:28 TP3] Init torch distributed ends. mem usage=0.73 GB\n",
      "[2025-11-03 02:33:28 TP2] Init torch distributed ends. mem usage=0.73 GB\n",
      "[2025-11-03 02:33:28 TP1] Init torch distributed ends. mem usage=0.73 GB\n",
      "[2025-11-03 02:33:28 TP0] MOE_RUNNER_BACKEND is not initialized, the backend will be automatically selected\n",
      "/root/anaconda3/envs/PRewrite/lib/python3.12/site-packages/pydantic/_internal/_generate_schema.py:2249: UnsupportedFieldAttributeWarning: The 'repr' attribute with value False was provided to the `Field()` function, which has no effect in the context it was used. 'repr' is field-specific metadata, and can only be attached to a model field using `Annotated` metadata or by assignment. This may have happened because an `Annotated` type alias using the `type` statement was used, or if the `Field()` function was attached to a single member of a union type.\n",
      "  warnings.warn(\n",
      "/root/anaconda3/envs/PRewrite/lib/python3.12/site-packages/pydantic/_internal/_generate_schema.py:2249: UnsupportedFieldAttributeWarning: The 'frozen' attribute with value True was provided to the `Field()` function, which has no effect in the context it was used. 'frozen' is field-specific metadata, and can only be attached to a model field using `Annotated` metadata or by assignment. This may have happened because an `Annotated` type alias using the `type` statement was used, or if the `Field()` function was attached to a single member of a union type.\n",
      "  warnings.warn(\n",
      "/root/anaconda3/envs/PRewrite/lib/python3.12/site-packages/pydantic/_internal/_generate_schema.py:2249: UnsupportedFieldAttributeWarning: The 'repr' attribute with value False was provided to the `Field()` function, which has no effect in the context it was used. 'repr' is field-specific metadata, and can only be attached to a model field using `Annotated` metadata or by assignment. This may have happened because an `Annotated` type alias using the `type` statement was used, or if the `Field()` function was attached to a single member of a union type.\n",
      "  warnings.warn(\n",
      "/root/anaconda3/envs/PRewrite/lib/python3.12/site-packages/pydantic/_internal/_generate_schema.py:2249: UnsupportedFieldAttributeWarning: The 'frozen' attribute with value True was provided to the `Field()` function, which has no effect in the context it was used. 'frozen' is field-specific metadata, and can only be attached to a model field using `Annotated` metadata or by assignment. This may have happened because an `Annotated` type alias using the `type` statement was used, or if the `Field()` function was attached to a single member of a union type.\n",
      "  warnings.warn(\n",
      "/root/anaconda3/envs/PRewrite/lib/python3.12/site-packages/pydantic/_internal/_generate_schema.py:2249: UnsupportedFieldAttributeWarning: The 'repr' attribute with value False was provided to the `Field()` function, which has no effect in the context it was used. 'repr' is field-specific metadata, and can only be attached to a model field using `Annotated` metadata or by assignment. This may have happened because an `Annotated` type alias using the `type` statement was used, or if the `Field()` function was attached to a single member of a union type.\n",
      "  warnings.warn(\n",
      "/root/anaconda3/envs/PRewrite/lib/python3.12/site-packages/pydantic/_internal/_generate_schema.py:2249: UnsupportedFieldAttributeWarning: The 'frozen' attribute with value True was provided to the `Field()` function, which has no effect in the context it was used. 'frozen' is field-specific metadata, and can only be attached to a model field using `Annotated` metadata or by assignment. This may have happened because an `Annotated` type alias using the `type` statement was used, or if the `Field()` function was attached to a single member of a union type.\n",
      "  warnings.warn(\n",
      "/root/anaconda3/envs/PRewrite/lib/python3.12/site-packages/pydantic/_internal/_generate_schema.py:2249: UnsupportedFieldAttributeWarning: The 'repr' attribute with value False was provided to the `Field()` function, which has no effect in the context it was used. 'repr' is field-specific metadata, and can only be attached to a model field using `Annotated` metadata or by assignment. This may have happened because an `Annotated` type alias using the `type` statement was used, or if the `Field()` function was attached to a single member of a union type.\n",
      "  warnings.warn(\n",
      "/root/anaconda3/envs/PRewrite/lib/python3.12/site-packages/pydantic/_internal/_generate_schema.py:2249: UnsupportedFieldAttributeWarning: The 'frozen' attribute with value True was provided to the `Field()` function, which has no effect in the context it was used. 'frozen' is field-specific metadata, and can only be attached to a model field using `Annotated` metadata or by assignment. This may have happened because an `Annotated` type alias using the `type` statement was used, or if the `Field()` function was attached to a single member of a union type.\n",
      "  warnings.warn(\n",
      "/root/anaconda3/envs/PRewrite/lib/python3.12/site-packages/pydantic/_internal/_generate_schema.py:2249: UnsupportedFieldAttributeWarning: The 'repr' attribute with value False was provided to the `Field()` function, which has no effect in the context it was used. 'repr' is field-specific metadata, and can only be attached to a model field using `Annotated` metadata or by assignment. This may have happened because an `Annotated` type alias using the `type` statement was used, or if the `Field()` function was attached to a single member of a union type.\n",
      "  warnings.warn(\n",
      "/root/anaconda3/envs/PRewrite/lib/python3.12/site-packages/pydantic/_internal/_generate_schema.py:2249: UnsupportedFieldAttributeWarning: The 'frozen' attribute with value True was provided to the `Field()` function, which has no effect in the context it was used. 'frozen' is field-specific metadata, and can only be attached to a model field using `Annotated` metadata or by assignment. This may have happened because an `Annotated` type alias using the `type` statement was used, or if the `Field()` function was attached to a single member of a union type.\n",
      "  warnings.warn(\n",
      "/root/anaconda3/envs/PRewrite/lib/python3.12/site-packages/pydantic/_internal/_generate_schema.py:2249: UnsupportedFieldAttributeWarning: The 'repr' attribute with value False was provided to the `Field()` function, which has no effect in the context it was used. 'repr' is field-specific metadata, and can only be attached to a model field using `Annotated` metadata or by assignment. This may have happened because an `Annotated` type alias using the `type` statement was used, or if the `Field()` function was attached to a single member of a union type.\n",
      "  warnings.warn(\n",
      "/root/anaconda3/envs/PRewrite/lib/python3.12/site-packages/pydantic/_internal/_generate_schema.py:2249: UnsupportedFieldAttributeWarning: The 'frozen' attribute with value True was provided to the `Field()` function, which has no effect in the context it was used. 'frozen' is field-specific metadata, and can only be attached to a model field using `Annotated` metadata or by assignment. This may have happened because an `Annotated` type alias using the `type` statement was used, or if the `Field()` function was attached to a single member of a union type.\n",
      "  warnings.warn(\n",
      "/root/anaconda3/envs/PRewrite/lib/python3.12/site-packages/pydantic/_internal/_generate_schema.py:2249: UnsupportedFieldAttributeWarning: The 'repr' attribute with value False was provided to the `Field()` function, which has no effect in the context it was used. 'repr' is field-specific metadata, and can only be attached to a model field using `Annotated` metadata or by assignment. This may have happened because an `Annotated` type alias using the `type` statement was used, or if the `Field()` function was attached to a single member of a union type.\n",
      "  warnings.warn(\n",
      "/root/anaconda3/envs/PRewrite/lib/python3.12/site-packages/pydantic/_internal/_generate_schema.py:2249: UnsupportedFieldAttributeWarning: The 'frozen' attribute with value True was provided to the `Field()` function, which has no effect in the context it was used. 'frozen' is field-specific metadata, and can only be attached to a model field using `Annotated` metadata or by assignment. This may have happened because an `Annotated` type alias using the `type` statement was used, or if the `Field()` function was attached to a single member of a union type.\n",
      "  warnings.warn(\n",
      "/root/anaconda3/envs/PRewrite/lib/python3.12/site-packages/pydantic/_internal/_generate_schema.py:2249: UnsupportedFieldAttributeWarning: The 'repr' attribute with value False was provided to the `Field()` function, which has no effect in the context it was used. 'repr' is field-specific metadata, and can only be attached to a model field using `Annotated` metadata or by assignment. This may have happened because an `Annotated` type alias using the `type` statement was used, or if the `Field()` function was attached to a single member of a union type.\n",
      "  warnings.warn(\n",
      "/root/anaconda3/envs/PRewrite/lib/python3.12/site-packages/pydantic/_internal/_generate_schema.py:2249: UnsupportedFieldAttributeWarning: The 'frozen' attribute with value True was provided to the `Field()` function, which has no effect in the context it was used. 'frozen' is field-specific metadata, and can only be attached to a model field using `Annotated` metadata or by assignment. This may have happened because an `Annotated` type alias using the `type` statement was used, or if the `Field()` function was attached to a single member of a union type.\n",
      "  warnings.warn(\n",
      "[2025-11-03 02:33:32 TP6] Load weight begin. avail mem=74.33 GB\n",
      "[2025-11-03 02:33:32 TP0] Load weight begin. avail mem=74.43 GB\n",
      "[2025-11-03 02:33:32 TP2] Load weight begin. avail mem=74.33 GB\n",
      "[2025-11-03 02:33:32 TP4] Load weight begin. avail mem=74.33 GB\n",
      "[2025-11-03 02:33:32 TP7] Load weight begin. avail mem=74.43 GB\n",
      "[2025-11-03 02:33:32 TP5] Load weight begin. avail mem=74.33 GB\n",
      "[2025-11-03 02:33:32 TP3] Load weight begin. avail mem=74.33 GB\n",
      "[2025-11-03 02:33:32 TP1] Load weight begin. avail mem=74.33 GB\n",
      "Loading safetensors checkpoint shards:   0% Completed | 0/17 [00:00<?, ?it/s]\n",
      "Loading safetensors checkpoint shards:   6% Completed | 1/17 [00:07<01:52,  7.02s/it]\n",
      "Loading safetensors checkpoint shards:  12% Completed | 2/17 [00:13<01:41,  6.78s/it]\n",
      "Loading safetensors checkpoint shards:  18% Completed | 3/17 [00:17<01:15,  5.37s/it]\n",
      "Loading safetensors checkpoint shards:  24% Completed | 4/17 [00:23<01:14,  5.72s/it]\n",
      "Loading safetensors checkpoint shards:  29% Completed | 5/17 [00:29<01:11,  5.92s/it]\n",
      "Loading safetensors checkpoint shards:  35% Completed | 6/17 [00:36<01:07,  6.12s/it]\n",
      "Loading safetensors checkpoint shards:  41% Completed | 7/17 [00:42<01:01,  6.17s/it]\n",
      "Loading safetensors checkpoint shards:  47% Completed | 8/17 [00:49<00:57,  6.38s/it]\n",
      "Loading safetensors checkpoint shards:  53% Completed | 9/17 [00:55<00:50,  6.36s/it]\n",
      "Loading safetensors checkpoint shards:  59% Completed | 10/17 [01:02<00:45,  6.47s/it]\n",
      "Loading safetensors checkpoint shards:  65% Completed | 11/17 [01:08<00:38,  6.34s/it]\n",
      "Loading safetensors checkpoint shards:  71% Completed | 12/17 [01:15<00:32,  6.44s/it]\n",
      "Loading safetensors checkpoint shards:  76% Completed | 13/17 [01:21<00:25,  6.34s/it]\n",
      "Loading safetensors checkpoint shards:  82% Completed | 14/17 [01:27<00:19,  6.43s/it]\n",
      "Loading safetensors checkpoint shards:  88% Completed | 15/17 [01:33<00:12,  6.05s/it]\n",
      "Loading safetensors checkpoint shards:  94% Completed | 16/17 [01:39<00:06,  6.12s/it]\n",
      "Loading safetensors checkpoint shards: 100% Completed | 17/17 [01:45<00:00,  6.13s/it]\n",
      "Loading safetensors checkpoint shards: 100% Completed | 17/17 [01:45<00:00,  6.21s/it]\n",
      "\n",
      "[2025-11-03 02:35:18 TP1] Load weight end. type=Qwen3ForCausalLM, dtype=torch.bfloat16, avail mem=66.46 GB, mem usage=7.88 GB.\n",
      "[2025-11-03 02:35:18 TP2] Load weight end. type=Qwen3ForCausalLM, dtype=torch.bfloat16, avail mem=66.46 GB, mem usage=7.88 GB.\n",
      "[2025-11-03 02:35:18 TP0] Load weight end. type=Qwen3ForCausalLM, dtype=torch.bfloat16, avail mem=66.55 GB, mem usage=7.88 GB.\n",
      "[2025-11-03 02:35:18 TP3] Load weight end. type=Qwen3ForCausalLM, dtype=torch.bfloat16, avail mem=66.46 GB, mem usage=7.88 GB.\n",
      "[2025-11-03 02:35:18 TP4] Load weight end. type=Qwen3ForCausalLM, dtype=torch.bfloat16, avail mem=66.46 GB, mem usage=7.88 GB.\n",
      "[2025-11-03 02:35:18 TP6] Load weight end. type=Qwen3ForCausalLM, dtype=torch.bfloat16, avail mem=66.46 GB, mem usage=7.88 GB.\n",
      "[2025-11-03 02:35:18 TP5] Load weight end. type=Qwen3ForCausalLM, dtype=torch.bfloat16, avail mem=66.46 GB, mem usage=7.88 GB.\n",
      "[2025-11-03 02:35:18 TP7] Load weight end. type=Qwen3ForCausalLM, dtype=torch.bfloat16, avail mem=66.55 GB, mem usage=7.88 GB.\n",
      "[2025-11-03 02:35:18 TP0] Using KV cache dtype: torch.bfloat16\n",
      "[2025-11-03 02:35:18 TP0] KV Cache is allocated. #tokens: 20480, K size: 0.31 GB, V size: 0.31 GB\n",
      "[2025-11-03 02:35:18 TP7] KV Cache is allocated. #tokens: 20480, K size: 0.31 GB, V size: 0.31 GB\n",
      "[2025-11-03 02:35:18 TP5] KV Cache is allocated. #tokens: 20480, K size: 0.31 GB, V size: 0.31 GB\n",
      "[2025-11-03 02:35:18 TP2] KV Cache is allocated. #tokens: 20480, K size: 0.31 GB, V size: 0.31 GB\n",
      "[2025-11-03 02:35:18 TP0] Memory pool end. avail mem=65.69 GB\n",
      "[2025-11-03 02:35:18 TP6] KV Cache is allocated. #tokens: 20480, K size: 0.31 GB, V size: 0.31 GB\n",
      "[2025-11-03 02:35:18 TP4] KV Cache is allocated. #tokens: 20480, K size: 0.31 GB, V size: 0.31 GB\n",
      "[2025-11-03 02:35:18 TP7] Memory pool end. avail mem=65.69 GB\n",
      "[2025-11-03 02:35:18 TP5] Memory pool end. avail mem=65.60 GB\n",
      "[2025-11-03 02:35:18 TP2] Memory pool end. avail mem=65.60 GB\n",
      "[2025-11-03 02:35:18 TP3] KV Cache is allocated. #tokens: 20480, K size: 0.31 GB, V size: 0.31 GB\n",
      "[2025-11-03 02:35:18 TP1] KV Cache is allocated. #tokens: 20480, K size: 0.31 GB, V size: 0.31 GB\n",
      "[2025-11-03 02:35:18 TP4] Memory pool end. avail mem=65.60 GB\n",
      "[2025-11-03 02:35:18 TP6] Memory pool end. avail mem=65.60 GB\n",
      "[2025-11-03 02:35:18 TP3] Memory pool end. avail mem=65.60 GB\n",
      "[2025-11-03 02:35:18 TP1] Memory pool end. avail mem=65.60 GB\n",
      "[2025-11-03 02:35:19 TP7] Capture cuda graph begin. This can take up to several minutes. avail mem=65.07 GB\n",
      "[2025-11-03 02:35:19 TP0] Capture cuda graph begin. This can take up to several minutes. avail mem=65.07 GB\n",
      "[2025-11-03 02:35:19 TP0] Capture cuda graph bs [1, 2, 4, 8, 12, 16, 24, 32, 40, 48, 56, 64, 72, 80, 88, 96, 104, 112, 120, 128]\n",
      "[2025-11-03 02:35:19 TP4] Capture cuda graph begin. This can take up to several minutes. avail mem=64.98 GB\n",
      "[2025-11-03 02:35:19 TP5] Capture cuda graph begin. This can take up to several minutes. avail mem=64.98 GB\n",
      "[2025-11-03 02:35:19 TP1] Capture cuda graph begin. This can take up to several minutes. avail mem=64.98 GB\n",
      "[2025-11-03 02:35:19 TP2] Capture cuda graph begin. This can take up to several minutes. avail mem=64.98 GB\n",
      "[2025-11-03 02:35:19 TP3] Capture cuda graph begin. This can take up to several minutes. avail mem=64.98 GB\n",
      "[2025-11-03 02:35:19 TP6] Capture cuda graph begin. This can take up to several minutes. avail mem=64.98 GB\n",
      "Capturing batches (bs=128 avail_mem=64.96 GB):   0%|          | 0/20 [00:48<?, ?it/s]\n",
      "[2025-11-03 02:36:08 TP0] Registering 0 cuda graph addresses\n",
      "[2025-11-03 02:36:08 TP2] Scheduler hit an exception: Traceback (most recent call last):\n",
      "  File \"/root/anaconda3/envs/PRewrite/lib/python3.12/site-packages/flashinfer/jit/cpp_ext.py\", line 282, in run_ninja\n",
      "    subprocess.run(\n",
      "  File \"/root/anaconda3/envs/PRewrite/lib/python3.12/subprocess.py\", line 571, in run\n",
      "    raise CalledProcessError(retcode, process.args,\n",
      "subprocess.CalledProcessError: Command '['ninja', '-v', '-C', '/root/.cache/flashinfer/80/cached_ops', '-f', '/root/.cache/flashinfer/80/cached_ops/batch_prefill_with_kv_cache_dtype_q_bf16_dtype_kv_bf16_dtype_o_bf16_dtype_idx_i32_head_dim_qk_128_head_dim_vo_128_posenc_0_use_swa_False_use_logits_cap_False_f16qk_False/build.ninja']' returned non-zero exit status 2.\n",
      "\n",
      "The above exception was the direct cause of the following exception:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/root/anaconda3/envs/PRewrite/lib/python3.12/site-packages/sglang/srt/model_executor/cuda_graph_runner.py\", line 366, in __init__\n",
      "    self.capture()\n",
      "  File \"/root/anaconda3/envs/PRewrite/lib/python3.12/site-packages/sglang/srt/model_executor/cuda_graph_runner.py\", line 485, in capture\n",
      "    ) = self.capture_one_batch_size(bs, forward)\n",
      "        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/root/anaconda3/envs/PRewrite/lib/python3.12/site-packages/sglang/srt/model_executor/cuda_graph_runner.py\", line 625, in capture_one_batch_size\n",
      "    self.model_runner.attn_backend.init_forward_metadata_capture_cuda_graph(\n",
      "  File \"/root/anaconda3/envs/PRewrite/lib/python3.12/site-packages/sglang/srt/layers/attention/flashinfer_backend.py\", line 543, in init_forward_metadata_capture_cuda_graph\n",
      "    self.indices_updater_decode.update(\n",
      "  File \"/root/anaconda3/envs/PRewrite/lib/python3.12/site-packages/sglang/srt/layers/attention/flashinfer_backend.py\", line 886, in update_single_wrapper\n",
      "    self.call_begin_forward(\n",
      "  File \"/root/anaconda3/envs/PRewrite/lib/python3.12/site-packages/sglang/srt/layers/attention/flashinfer_backend.py\", line 1067, in call_begin_forward\n",
      "    wrapper.begin_forward(\n",
      "  File \"/root/anaconda3/envs/PRewrite/lib/python3.12/site-packages/flashinfer/decode.py\", line 1030, in plan\n",
      "    self._cached_module = get_batch_prefill_module(\n",
      "                          ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/root/anaconda3/envs/PRewrite/lib/python3.12/site-packages/flashinfer/prefill.py\", line 374, in get_batch_prefill_module\n",
      "    module = gen_batch_prefill_module(backend, *args).build_and_load()\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/root/anaconda3/envs/PRewrite/lib/python3.12/site-packages/flashinfer/jit/core.py\", line 277, in build_and_load\n",
      "    self.build(verbose, need_lock=False)\n",
      "  File \"/root/anaconda3/envs/PRewrite/lib/python3.12/site-packages/flashinfer/jit/core.py\", line 263, in build\n",
      "    run_ninja(jit_env.FLASHINFER_JIT_DIR, self.ninja_path, verbose)\n",
      "  File \"/root/anaconda3/envs/PRewrite/lib/python3.12/site-packages/flashinfer/jit/cpp_ext.py\", line 294, in run_ninja\n",
      "    raise RuntimeError(msg) from e\n",
      "RuntimeError: Ninja build failed. Ninja output:\n",
      "ninja: Entering directory `/root/.cache/flashinfer/80/cached_ops'\n",
      "[1/3] /usr/local/cuda/bin/nvcc --generate-dependencies-with-compile --dependency-output batch_prefill_with_kv_cache_dtype_q_bf16_dtype_kv_bf16_dtype_o_bf16_dtype_idx_i32_head_dim_qk_128_head_dim_vo_128_posenc_0_use_swa_False_use_logits_cap_False_f16qk_False/batch_prefill.cuda.o.d -DTORCH_EXTENSION_NAME=batch_prefill_with_kv_cache_dtype_q_bf16_dtype_kv_bf16_dtype_o_bf16_dtype_idx_i32_head_dim_qk_128_head_dim_vo_128_posenc_0_use_swa_False_use_logits_cap_False_f16qk_False -DTORCH_API_INCLUDE_EXTENSION_H -DPy_LIMITED_API=0x03090000 -DPYBIND11_COMPILER_TYPE=\\\"_gcc\\\" -DPYBIND11_STDLIB=\\\"_libstdcpp\\\" -DPYBIND11_BUILD_ABI=\\\"_cxxabi1018\\\" -D_GLIBCXX_USE_CXX11_ABI=1 -isystem /root/anaconda3/envs/llm-serve/include/python3.12 -isystem /root/anaconda3/envs/llm-serve/lib/python3.12/site-packages/torch/include -isystem /root/anaconda3/envs/llm-serve/lib/python3.12/site-packages/torch/include/torch/csrc/api/include -isystem /usr/local/cuda/include -isystem /usr/local/cuda/include/cccl -isystem /root/anaconda3/envs/llm-serve/lib/python3.12/site-packages/flashinfer/data/include -isystem /root/anaconda3/envs/llm-serve/lib/python3.12/site-packages/flashinfer/data/csrc -isystem /root/anaconda3/envs/llm-serve/lib/python3.12/site-packages/flashinfer/data/cutlass/include -isystem /root/anaconda3/envs/llm-serve/lib/python3.12/site-packages/flashinfer/data/cutlass/tools/util/include -isystem /root/anaconda3/envs/llm-serve/lib/python3.12/site-packages/flashinfer/data/spdlog/include --compiler-options=-fPIC --expt-relaxed-constexpr -gencode=arch=compute_80,code=sm_80 -DFLASHINFER_ENABLE_FP8_E8M0 -DFLASHINFER_ENABLE_FP4_E2M1 -O3 -std=c++17 --threads=1 -use_fast_math -DFLASHINFER_ENABLE_F16 -DFLASHINFER_ENABLE_BF16 -DFLASHINFER_ENABLE_FP8_E4M3 -DFLASHINFER_ENABLE_FP8_E5M2 -DNDEBUG -lineinfo -c /root/.cache/flashinfer/80/generated/batch_prefill_with_kv_cache_dtype_q_bf16_dtype_kv_bf16_dtype_o_bf16_dtype_idx_i32_head_dim_qk_128_head_dim_vo_128_posenc_0_use_swa_False_use_logits_cap_False_f16qk_False/batch_prefill.cu -o batch_prefill_with_kv_cache_dtype_q_bf16_dtype_kv_bf16_dtype_o_bf16_dtype_idx_i32_head_dim_qk_128_head_dim_vo_128_posenc_0_use_swa_False_use_logits_cap_False_f16qk_False/batch_prefill.cuda.o \n",
      "\u001b[31mFAILED: [code=1] \u001b[0mbatch_prefill_with_kv_cache_dtype_q_bf16_dtype_kv_bf16_dtype_o_bf16_dtype_idx_i32_head_dim_qk_128_head_dim_vo_128_posenc_0_use_swa_False_use_logits_cap_False_f16qk_False/batch_prefill.cuda.o \n",
      "/usr/local/cuda/bin/nvcc --generate-dependencies-with-compile --dependency-output batch_prefill_with_kv_cache_dtype_q_bf16_dtype_kv_bf16_dtype_o_bf16_dtype_idx_i32_head_dim_qk_128_head_dim_vo_128_posenc_0_use_swa_False_use_logits_cap_False_f16qk_False/batch_prefill.cuda.o.d -DTORCH_EXTENSION_NAME=batch_prefill_with_kv_cache_dtype_q_bf16_dtype_kv_bf16_dtype_o_bf16_dtype_idx_i32_head_dim_qk_128_head_dim_vo_128_posenc_0_use_swa_False_use_logits_cap_False_f16qk_False -DTORCH_API_INCLUDE_EXTENSION_H -DPy_LIMITED_API=0x03090000 -DPYBIND11_COMPILER_TYPE=\\\"_gcc\\\" -DPYBIND11_STDLIB=\\\"_libstdcpp\\\" -DPYBIND11_BUILD_ABI=\\\"_cxxabi1018\\\" -D_GLIBCXX_USE_CXX11_ABI=1 -isystem /root/anaconda3/envs/llm-serve/include/python3.12 -isystem /root/anaconda3/envs/llm-serve/lib/python3.12/site-packages/torch/include -isystem /root/anaconda3/envs/llm-serve/lib/python3.12/site-packages/torch/include/torch/csrc/api/include -isystem /usr/local/cuda/include -isystem /usr/local/cuda/include/cccl -isystem /root/anaconda3/envs/llm-serve/lib/python3.12/site-packages/flashinfer/data/include -isystem /root/anaconda3/envs/llm-serve/lib/python3.12/site-packages/flashinfer/data/csrc -isystem /root/anaconda3/envs/llm-serve/lib/python3.12/site-packages/flashinfer/data/cutlass/include -isystem /root/anaconda3/envs/llm-serve/lib/python3.12/site-packages/flashinfer/data/cutlass/tools/util/include -isystem /root/anaconda3/envs/llm-serve/lib/python3.12/site-packages/flashinfer/data/spdlog/include --compiler-options=-fPIC --expt-relaxed-constexpr -gencode=arch=compute_80,code=sm_80 -DFLASHINFER_ENABLE_FP8_E8M0 -DFLASHINFER_ENABLE_FP4_E2M1 -O3 -std=c++17 --threads=1 -use_fast_math -DFLASHINFER_ENABLE_F16 -DFLASHINFER_ENABLE_BF16 -DFLASHINFER_ENABLE_FP8_E4M3 -DFLASHINFER_ENABLE_FP8_E5M2 -DNDEBUG -lineinfo -c /root/.cache/flashinfer/80/generated/batch_prefill_with_kv_cache_dtype_q_bf16_dtype_kv_bf16_dtype_o_bf16_dtype_idx_i32_head_dim_qk_128_head_dim_vo_128_posenc_0_use_swa_False_use_logits_cap_False_f16qk_False/batch_prefill.cu -o batch_prefill_with_kv_cache_dtype_q_bf16_dtype_kv_bf16_dtype_o_bf16_dtype_idx_i32_head_dim_qk_128_head_dim_vo_128_posenc_0_use_swa_False_use_logits_cap_False_f16qk_False/batch_prefill.cuda.o \n",
      "/root/.cache/flashinfer/80/generated/batch_prefill_with_kv_cache_dtype_q_bf16_dtype_kv_bf16_dtype_o_bf16_dtype_idx_i32_head_dim_qk_128_head_dim_vo_128_posenc_0_use_swa_False_use_logits_cap_False_f16qk_False/batch_prefill.cu:21:10: fatal error: tvm/ffi/container/array.h: No such file or directory\n",
      "   21 | #include \"tvm/ffi/container/array.h\"\n",
      "      |          ^~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "compilation terminated.\n",
      "[2/3] /usr/local/cuda/bin/nvcc --generate-dependencies-with-compile --dependency-output batch_prefill_with_kv_cache_dtype_q_bf16_dtype_kv_bf16_dtype_o_bf16_dtype_idx_i32_head_dim_qk_128_head_dim_vo_128_posenc_0_use_swa_False_use_logits_cap_False_f16qk_False/batch_prefill_jit_pybind.cuda.o.d -DTORCH_EXTENSION_NAME=batch_prefill_with_kv_cache_dtype_q_bf16_dtype_kv_bf16_dtype_o_bf16_dtype_idx_i32_head_dim_qk_128_head_dim_vo_128_posenc_0_use_swa_False_use_logits_cap_False_f16qk_False -DTORCH_API_INCLUDE_EXTENSION_H -DPy_LIMITED_API=0x03090000 -DPYBIND11_COMPILER_TYPE=\\\"_gcc\\\" -DPYBIND11_STDLIB=\\\"_libstdcpp\\\" -DPYBIND11_BUILD_ABI=\\\"_cxxabi1018\\\" -D_GLIBCXX_USE_CXX11_ABI=1 -isystem /root/anaconda3/envs/llm-serve/include/python3.12 -isystem /root/anaconda3/envs/llm-serve/lib/python3.12/site-packages/torch/include -isystem /root/anaconda3/envs/llm-serve/lib/python3.12/site-packages/torch/include/torch/csrc/api/include -isystem /usr/local/cuda/include -isystem /usr/local/cuda/include/cccl -isystem /root/anaconda3/envs/llm-serve/lib/python3.12/site-packages/flashinfer/data/include -isystem /root/anaconda3/envs/llm-serve/lib/python3.12/site-packages/flashinfer/data/csrc -isystem /root/anaconda3/envs/llm-serve/lib/python3.12/site-packages/flashinfer/data/cutlass/include -isystem /root/anaconda3/envs/llm-serve/lib/python3.12/site-packages/flashinfer/data/cutlass/tools/util/include -isystem /root/anaconda3/envs/llm-serve/lib/python3.12/site-packages/flashinfer/data/spdlog/include --compiler-options=-fPIC --expt-relaxed-constexpr -gencode=arch=compute_80,code=sm_80 -DFLASHINFER_ENABLE_FP8_E8M0 -DFLASHINFER_ENABLE_FP4_E2M1 -O3 -std=c++17 --threads=1 -use_fast_math -DFLASHINFER_ENABLE_F16 -DFLASHINFER_ENABLE_BF16 -DFLASHINFER_ENABLE_FP8_E4M3 -DFLASHINFER_ENABLE_FP8_E5M2 -DNDEBUG -lineinfo -c /root/.cache/flashinfer/80/generated/batch_prefill_with_kv_cache_dtype_q_bf16_dtype_kv_bf16_dtype_o_bf16_dtype_idx_i32_head_dim_qk_128_head_dim_vo_128_posenc_0_use_swa_False_use_logits_cap_False_f16qk_False/batch_prefill_jit_pybind.cu -o batch_prefill_with_kv_cache_dtype_q_bf16_dtype_kv_bf16_dtype_o_bf16_dtype_idx_i32_head_dim_qk_128_head_dim_vo_128_posenc_0_use_swa_False_use_logits_cap_False_f16qk_False/batch_prefill_jit_pybind.cuda.o \n",
      "\u001b[31mFAILED: [code=2] \u001b[0mbatch_prefill_with_kv_cache_dtype_q_bf16_dtype_kv_bf16_dtype_o_bf16_dtype_idx_i32_head_dim_qk_128_head_dim_vo_128_posenc_0_use_swa_False_use_logits_cap_False_f16qk_False/batch_prefill_jit_pybind.cuda.o \n",
      "/usr/local/cuda/bin/nvcc --generate-dependencies-with-compile --dependency-output batch_prefill_with_kv_cache_dtype_q_bf16_dtype_kv_bf16_dtype_o_bf16_dtype_idx_i32_head_dim_qk_128_head_dim_vo_128_posenc_0_use_swa_False_use_logits_cap_False_f16qk_False/batch_prefill_jit_pybind.cuda.o.d -DTORCH_EXTENSION_NAME=batch_prefill_with_kv_cache_dtype_q_bf16_dtype_kv_bf16_dtype_o_bf16_dtype_idx_i32_head_dim_qk_128_head_dim_vo_128_posenc_0_use_swa_False_use_logits_cap_False_f16qk_False -DTORCH_API_INCLUDE_EXTENSION_H -DPy_LIMITED_API=0x03090000 -DPYBIND11_COMPILER_TYPE=\\\"_gcc\\\" -DPYBIND11_STDLIB=\\\"_libstdcpp\\\" -DPYBIND11_BUILD_ABI=\\\"_cxxabi1018\\\" -D_GLIBCXX_USE_CXX11_ABI=1 -isystem /root/anaconda3/envs/llm-serve/include/python3.12 -isystem /root/anaconda3/envs/llm-serve/lib/python3.12/site-packages/torch/include -isystem /root/anaconda3/envs/llm-serve/lib/python3.12/site-packages/torch/include/torch/csrc/api/include -isystem /usr/local/cuda/include -isystem /usr/local/cuda/include/cccl -isystem /root/anaconda3/envs/llm-serve/lib/python3.12/site-packages/flashinfer/data/include -isystem /root/anaconda3/envs/llm-serve/lib/python3.12/site-packages/flashinfer/data/csrc -isystem /root/anaconda3/envs/llm-serve/lib/python3.12/site-packages/flashinfer/data/cutlass/include -isystem /root/anaconda3/envs/llm-serve/lib/python3.12/site-packages/flashinfer/data/cutlass/tools/util/include -isystem /root/anaconda3/envs/llm-serve/lib/python3.12/site-packages/flashinfer/data/spdlog/include --compiler-options=-fPIC --expt-relaxed-constexpr -gencode=arch=compute_80,code=sm_80 -DFLASHINFER_ENABLE_FP8_E8M0 -DFLASHINFER_ENABLE_FP4_E2M1 -O3 -std=c++17 --threads=1 -use_fast_math -DFLASHINFER_ENABLE_F16 -DFLASHINFER_ENABLE_BF16 -DFLASHINFER_ENABLE_FP8_E4M3 -DFLASHINFER_ENABLE_FP8_E5M2 -DNDEBUG -lineinfo -c /root/.cache/flashinfer/80/generated/batch_prefill_with_kv_cache_dtype_q_bf16_dtype_kv_bf16_dtype_o_bf16_dtype_idx_i32_head_dim_qk_128_head_dim_vo_128_posenc_0_use_swa_False_use_logits_cap_False_f16qk_False/batch_prefill_jit_pybind.cu -o batch_prefill_with_kv_cache_dtype_q_bf16_dtype_kv_bf16_dtype_o_bf16_dtype_idx_i32_head_dim_qk_128_head_dim_vo_128_posenc_0_use_swa_False_use_logits_cap_False_f16qk_False/batch_prefill_jit_pybind.cuda.o \n",
      "/root/.cache/flashinfer/80/generated/batch_prefill_with_kv_cache_dtype_q_bf16_dtype_kv_bf16_dtype_o_bf16_dtype_idx_i32_head_dim_qk_128_head_dim_vo_128_posenc_0_use_swa_False_use_logits_cap_False_f16qk_False/batch_prefill_jit_pybind.cu(33): error: Optional is not a template\n",
      "                                        bool enable_pdl , Optional<ffi::Tensor> maybe_custom_mask, Optional<ffi::Tensor> maybe_mask_indptr, Optional<ffi::Tensor> maybe_alibi_slopes, Optional<ffi::Tensor> maybe_prefix_len_ptr, Optional<ffi::Tensor> maybe_token_pos_in_items_ptr, Optional<ffi::Tensor> maybe_max_item_len_ptr, double logits_soft_cap, double sm_scale, double rope_rcp_scale, double rope_rcp_theta, int64_t token_pos_in_items_len);\n",
      "                                                          ^\n",
      "\n",
      "/root/.cache/flashinfer/80/generated/batch_prefill_with_kv_cache_dtype_q_bf16_dtype_kv_bf16_dtype_o_bf16_dtype_idx_i32_head_dim_qk_128_head_dim_vo_128_posenc_0_use_swa_False_use_logits_cap_False_f16qk_False/batch_prefill_jit_pybind.cu(33): error: name followed by \"::\" must be a class or namespace name\n",
      "                                        bool enable_pdl , Optional<ffi::Tensor> maybe_custom_mask, Optional<ffi::Tensor> maybe_mask_indptr, Optional<ffi::Tensor> maybe_alibi_slopes, Optional<ffi::Tensor> maybe_prefix_len_ptr, Optional<ffi::Tensor> maybe_token_pos_in_items_ptr, Optional<ffi::Tensor> maybe_max_item_len_ptr, double logits_soft_cap, double sm_scale, double rope_rcp_scale, double rope_rcp_theta, int64_t token_pos_in_items_len);\n",
      "                                                                   ^\n",
      "\n",
      "/root/.cache/flashinfer/80/generated/batch_prefill_with_kv_cache_dtype_q_bf16_dtype_kv_bf16_dtype_o_bf16_dtype_idx_i32_head_dim_qk_128_head_dim_vo_128_posenc_0_use_swa_False_use_logits_cap_False_f16qk_False/batch_prefill_jit_pybind.cu(33): error: Optional is not a template\n",
      "                                        bool enable_pdl , Optional<ffi::Tensor> maybe_custom_mask, Optional<ffi::Tensor> maybe_mask_indptr, Optional<ffi::Tensor> maybe_alibi_slopes, Optional<ffi::Tensor> maybe_prefix_len_ptr, Optional<ffi::Tensor> maybe_token_pos_in_items_ptr, Optional<ffi::Tensor> maybe_max_item_len_ptr, double logits_soft_cap, double sm_scale, double rope_rcp_scale, double rope_rcp_theta, int64_t token_pos_in_items_len);\n",
      "                                                                                                   ^\n",
      "\n",
      "/root/.cache/flashinfer/80/generated/batch_prefill_with_kv_cache_dtype_q_bf16_dtype_kv_bf16_dtype_o_bf16_dtype_idx_i32_head_dim_qk_128_head_dim_vo_128_posenc_0_use_swa_False_use_logits_cap_False_f16qk_False/batch_prefill_jit_pybind.cu(33): error: name followed by \"::\" must be a class or namespace name\n",
      "                                        bool enable_pdl , Optional<ffi::Tensor> maybe_custom_mask, Optional<ffi::Tensor> maybe_mask_indptr, Optional<ffi::Tensor> maybe_alibi_slopes, Optional<ffi::Tensor> maybe_prefix_len_ptr, Optional<ffi::Tensor> maybe_token_pos_in_items_ptr, Optional<ffi::Tensor> maybe_max_item_len_ptr, double logits_soft_cap, double sm_scale, double rope_rcp_scale, double rope_rcp_theta, int64_t token_pos_in_items_len);\n",
      "                                                                                                            ^\n",
      "\n",
      "/root/.cache/flashinfer/80/generated/batch_prefill_with_kv_cache_dtype_q_bf16_dtype_kv_bf16_dtype_o_bf16_dtype_idx_i32_head_dim_qk_128_head_dim_vo_128_posenc_0_use_swa_False_use_logits_cap_False_f16qk_False/batch_prefill_jit_pybind.cu(33): error: Optional is not a template\n",
      "                                        bool enable_pdl , Optional<ffi::Tensor> maybe_custom_mask, Optional<ffi::Tensor> maybe_mask_indptr, Optional<ffi::Tensor> maybe_alibi_slopes, Optional<ffi::Tensor> maybe_prefix_len_ptr, Optional<ffi::Tensor> maybe_token_pos_in_items_ptr, Optional<ffi::Tensor> maybe_max_item_len_ptr, double logits_soft_cap, double sm_scale, double rope_rcp_scale, double rope_rcp_theta, int64_t token_pos_in_items_len);\n",
      "                                                                                                                                            ^\n",
      "\n",
      "/root/.cache/flashinfer/80/generated/batch_prefill_with_kv_cache_dtype_q_bf16_dtype_kv_bf16_dtype_o_bf16_dtype_idx_i32_head_dim_qk_128_head_dim_vo_128_posenc_0_use_swa_False_use_logits_cap_False_f16qk_False/batch_prefill_jit_pybind.cu(33): error: name followed by \"::\" must be a class or namespace name\n",
      "                                        bool enable_pdl , Optional<ffi::Tensor> maybe_custom_mask, Optional<ffi::Tensor> maybe_mask_indptr, Optional<ffi::Tensor> maybe_alibi_slopes, Optional<ffi::Tensor> maybe_prefix_len_ptr, Optional<ffi::Tensor> maybe_token_pos_in_items_ptr, Optional<ffi::Tensor> maybe_max_item_len_ptr, double logits_soft_cap, double sm_scale, double rope_rcp_scale, double rope_rcp_theta, int64_t token_pos_in_items_len);\n",
      "                                                                                                                                                     ^\n",
      "\n",
      "/root/.cache/flashinfer/80/generated/batch_prefill_with_kv_cache_dtype_q_bf16_dtype_kv_bf16_dtype_o_bf16_dtype_idx_i32_head_dim_qk_128_head_dim_vo_128_posenc_0_use_swa_False_use_logits_cap_False_f16qk_False/batch_prefill_jit_pybind.cu(33): error: Optional is not a template\n",
      "                                        bool enable_pdl , Optional<ffi::Tensor> maybe_custom_mask, Optional<ffi::Tensor> maybe_mask_indptr, Optional<ffi::Tensor> maybe_alibi_slopes, Optional<ffi::Tensor> maybe_prefix_len_ptr, Optional<ffi::Tensor> maybe_token_pos_in_items_ptr, Optional<ffi::Tensor> maybe_max_item_len_ptr, double logits_soft_cap, double sm_scale, double rope_rcp_scale, double rope_rcp_theta, int64_t token_pos_in_items_len);\n",
      "                                                                                                                                                                                      ^\n",
      "\n",
      "/root/.cache/flashinfer/80/generated/batch_prefill_with_kv_cache_dtype_q_bf16_dtype_kv_bf16_dtype_o_bf16_dtype_idx_i32_head_dim_qk_128_head_dim_vo_128_posenc_0_use_swa_False_use_logits_cap_False_f16qk_False/batch_prefill_jit_pybind.cu(33): error: name followed by \"::\" must be a class or namespace name\n",
      "                                        bool enable_pdl , Optional<ffi::Tensor> maybe_custom_mask, Optional<ffi::Tensor> maybe_mask_indptr, Optional<ffi::Tensor> maybe_alibi_slopes, Optional<ffi::Tensor> maybe_prefix_len_ptr, Optional<ffi::Tensor> maybe_token_pos_in_items_ptr, Optional<ffi::Tensor> maybe_max_item_len_ptr, double logits_soft_cap, double sm_scale, double rope_rcp_scale, double rope_rcp_theta, int64_t token_pos_in_items_len);\n",
      "                                                                                                                                                                                               ^\n",
      "\n",
      "/root/.cache/flashinfer/80/generated/batch_prefill_with_kv_cache_dtype_q_bf16_dtype_kv_bf16_dtype_o_bf16_dtype_idx_i32_head_dim_qk_128_head_dim_vo_128_posenc_0_use_swa_False_use_logits_cap_False_f16qk_False/batch_prefill_jit_pybind.cu(33): error: Optional is not a template\n",
      "                                        bool enable_pdl , Optional<ffi::Tensor> maybe_custom_mask, Optional<ffi::Tensor> maybe_mask_indptr, Optional<ffi::Tensor> maybe_alibi_slopes, Optional<ffi::Tensor> maybe_prefix_len_ptr, Optional<ffi::Tensor> maybe_token_pos_in_items_ptr, Optional<ffi::Tensor> maybe_max_item_len_ptr, double logits_soft_cap, double sm_scale, double rope_rcp_scale, double rope_rcp_theta, int64_t token_pos_in_items_len);\n",
      "                                                                                                                                                                                                                                  ^\n",
      "\n",
      "/root/.cache/flashinfer/80/generated/batch_prefill_with_kv_cache_dtype_q_bf16_dtype_kv_bf16_dtype_o_bf16_dtype_idx_i32_head_dim_qk_128_head_dim_vo_128_posenc_0_use_swa_False_use_logits_cap_False_f16qk_False/batch_prefill_jit_pybind.cu(33): error: name followed by \"::\" must be a class or namespace name\n",
      "                                        bool enable_pdl , Optional<ffi::Tensor> maybe_custom_mask, Optional<ffi::Tensor> maybe_mask_indptr, Optional<ffi::Tensor> maybe_alibi_slopes, Optional<ffi::Tensor> maybe_prefix_len_ptr, Optional<ffi::Tensor> maybe_token_pos_in_items_ptr, Optional<ffi::Tensor> maybe_max_item_len_ptr, double logits_soft_cap, double sm_scale, double rope_rcp_scale, double rope_rcp_theta, int64_t token_pos_in_items_len);\n",
      "                                                                                                                                                                                                                                           ^\n",
      "\n",
      "/root/.cache/flashinfer/80/generated/batch_prefill_with_kv_cache_dtype_q_bf16_dtype_kv_bf16_dtype_o_bf16_dtype_idx_i32_head_dim_qk_128_head_dim_vo_128_posenc_0_use_swa_False_use_logits_cap_False_f16qk_False/batch_prefill_jit_pybind.cu(33): error: Optional is not a template\n",
      "                                        bool enable_pdl , Optional<ffi::Tensor> maybe_custom_mask, Optional<ffi::Tensor> maybe_mask_indptr, Optional<ffi::Tensor> maybe_alibi_slopes, Optional<ffi::Tensor> maybe_prefix_len_ptr, Optional<ffi::Tensor> maybe_token_pos_in_items_ptr, Optional<ffi::Tensor> maybe_max_item_len_ptr, double logits_soft_cap, double sm_scale, double rope_rcp_scale, double rope_rcp_theta, int64_t token_pos_in_items_len);\n",
      "                                                                                                                                                                                                                                                                                      ^\n",
      "\n",
      "/root/.cache/flashinfer/80/generated/batch_prefill_with_kv_cache_dtype_q_bf16_dtype_kv_bf16_dtype_o_bf16_dtype_idx_i32_head_dim_qk_128_head_dim_vo_128_posenc_0_use_swa_False_use_logits_cap_False_f16qk_False/batch_prefill_jit_pybind.cu(33): error: name followed by \"::\" must be a class or namespace name\n",
      "                                        bool enable_pdl , Optional<ffi::Tensor> maybe_custom_mask, Optional<ffi::Tensor> maybe_mask_indptr, Optional<ffi::Tensor> maybe_alibi_slopes, Optional<ffi::Tensor> maybe_prefix_len_ptr, Optional<ffi::Tensor> maybe_token_pos_in_items_ptr, Optional<ffi::Tensor> maybe_max_item_len_ptr, double logits_soft_cap, double sm_scale, double rope_rcp_scale, double rope_rcp_theta, int64_t token_pos_in_items_len);\n",
      "                                                                                                                                                                                                                                                                                               ^\n",
      "\n",
      "/root/.cache/flashinfer/80/generated/batch_prefill_with_kv_cache_dtype_q_bf16_dtype_kv_bf16_dtype_o_bf16_dtype_idx_i32_head_dim_qk_128_head_dim_vo_128_posenc_0_use_swa_False_use_logits_cap_False_f16qk_False/batch_prefill_jit_pybind.cu(40): error: Optional is not a template\n",
      "      int64_t window_left, bool enable_pdl , Optional<ffi::Tensor> maybe_custom_mask, Optional<ffi::Tensor> maybe_mask_indptr, Optional<ffi::Tensor> maybe_alibi_slopes, Optional<ffi::Tensor> maybe_prefix_len_ptr, Optional<ffi::Tensor> maybe_token_pos_in_items_ptr, Optional<ffi::Tensor> maybe_max_item_len_ptr, double logits_soft_cap, double sm_scale, double rope_rcp_scale, double rope_rcp_theta, int64_t token_pos_in_items_len);\n",
      "                                             ^\n",
      "\n",
      "/root/.cache/flashinfer/80/generated/batch_prefill_with_kv_cache_dtype_q_bf16_dtype_kv_bf16_dtype_o_bf16_dtype_idx_i32_head_dim_qk_128_head_dim_vo_128_posenc_0_use_swa_False_use_logits_cap_False_f16qk_False/batch_prefill_jit_pybind.cu(40): error: name followed by \"::\" must be a class or namespace name\n",
      "      int64_t window_left, bool enable_pdl , Optional<ffi::Tensor> maybe_custom_mask, Optional<ffi::Tensor> maybe_mask_indptr, Optional<ffi::Tensor> maybe_alibi_slopes, Optional<ffi::Tensor> maybe_prefix_len_ptr, Optional<ffi::Tensor> maybe_token_pos_in_items_ptr, Optional<ffi::Tensor> maybe_max_item_len_ptr, double logits_soft_cap, double sm_scale, double rope_rcp_scale, double rope_rcp_theta, int64_t token_pos_in_items_len);\n",
      "                                                      ^\n",
      "\n",
      "/root/.cache/flashinfer/80/generated/batch_prefill_with_kv_cache_dtype_q_bf16_dtype_kv_bf16_dtype_o_bf16_dtype_idx_i32_head_dim_qk_128_head_dim_vo_128_posenc_0_use_swa_False_use_logits_cap_False_f16qk_False/batch_prefill_jit_pybind.cu(40): error: Optional is not a template\n",
      "      int64_t window_left, bool enable_pdl , Optional<ffi::Tensor> maybe_custom_mask, Optional<ffi::Tensor> maybe_mask_indptr, Optional<ffi::Tensor> maybe_alibi_slopes, Optional<ffi::Tensor> maybe_prefix_len_ptr, Optional<ffi::Tensor> maybe_token_pos_in_items_ptr, Optional<ffi::Tensor> maybe_max_item_len_ptr, double logits_soft_cap, double sm_scale, double rope_rcp_scale, double rope_rcp_theta, int64_t token_pos_in_items_len);\n",
      "                                                                                      ^\n",
      "\n",
      "/root/.cache/flashinfer/80/generated/batch_prefill_with_kv_cache_dtype_q_bf16_dtype_kv_bf16_dtype_o_bf16_dtype_idx_i32_head_dim_qk_128_head_dim_vo_128_posenc_0_use_swa_False_use_logits_cap_False_f16qk_False/batch_prefill_jit_pybind.cu(40): error: name followed by \"::\" must be a class or namespace name\n",
      "      int64_t window_left, bool enable_pdl , Optional<ffi::Tensor> maybe_custom_mask, Optional<ffi::Tensor> maybe_mask_indptr, Optional<ffi::Tensor> maybe_alibi_slopes, Optional<ffi::Tensor> maybe_prefix_len_ptr, Optional<ffi::Tensor> maybe_token_pos_in_items_ptr, Optional<ffi::Tensor> maybe_max_item_len_ptr, double logits_soft_cap, double sm_scale, double rope_rcp_scale, double rope_rcp_theta, int64_t token_pos_in_items_len);\n",
      "                                                                                               ^\n",
      "\n",
      "/root/.cache/flashinfer/80/generated/batch_prefill_with_kv_cache_dtype_q_bf16_dtype_kv_bf16_dtype_o_bf16_dtype_idx_i32_head_dim_qk_128_head_dim_vo_128_posenc_0_use_swa_False_use_logits_cap_False_f16qk_False/batch_prefill_jit_pybind.cu(40): error: Optional is not a template\n",
      "      int64_t window_left, bool enable_pdl , Optional<ffi::Tensor> maybe_custom_mask, Optional<ffi::Tensor> maybe_mask_indptr, Optional<ffi::Tensor> maybe_alibi_slopes, Optional<ffi::Tensor> maybe_prefix_len_ptr, Optional<ffi::Tensor> maybe_token_pos_in_items_ptr, Optional<ffi::Tensor> maybe_max_item_len_ptr, double logits_soft_cap, double sm_scale, double rope_rcp_scale, double rope_rcp_theta, int64_t token_pos_in_items_len);\n",
      "                                                                                                                               ^\n",
      "\n",
      "/root/.cache/flashinfer/80/generated/batch_prefill_with_kv_cache_dtype_q_bf16_dtype_kv_bf16_dtype_o_bf16_dtype_idx_i32_head_dim_qk_128_head_dim_vo_128_posenc_0_use_swa_False_use_logits_cap_False_f16qk_False/batch_prefill_jit_pybind.cu(40): error: name followed by \"::\" must be a class or namespace name\n",
      "      int64_t window_left, bool enable_pdl , Optional<ffi::Tensor> maybe_custom_mask, Optional<ffi::Tensor> maybe_mask_indptr, Optional<ffi::Tensor> maybe_alibi_slopes, Optional<ffi::Tensor> maybe_prefix_len_ptr, Optional<ffi::Tensor> maybe_token_pos_in_items_ptr, Optional<ffi::Tensor> maybe_max_item_len_ptr, double logits_soft_cap, double sm_scale, double rope_rcp_scale, double rope_rcp_theta, int64_t token_pos_in_items_len);\n",
      "                                                                                                                                        ^\n",
      "\n",
      "/root/.cache/flashinfer/80/generated/batch_prefill_with_kv_cache_dtype_q_bf16_dtype_kv_bf16_dtype_o_bf16_dtype_idx_i32_head_dim_qk_128_head_dim_vo_128_posenc_0_use_swa_False_use_logits_cap_False_f16qk_False/batch_prefill_jit_pybind.cu(40): error: Optional is not a template\n",
      "      int64_t window_left, bool enable_pdl , Optional<ffi::Tensor> maybe_custom_mask, Optional<ffi::Tensor> maybe_mask_indptr, Optional<ffi::Tensor> maybe_alibi_slopes, Optional<ffi::Tensor> maybe_prefix_len_ptr, Optional<ffi::Tensor> maybe_token_pos_in_items_ptr, Optional<ffi::Tensor> maybe_max_item_len_ptr, double logits_soft_cap, double sm_scale, double rope_rcp_scale, double rope_rcp_theta, int64_t token_pos_in_items_len);\n",
      "                                                                                                                                                                         ^\n",
      "\n",
      "/root/.cache/flashinfer/80/generated/batch_prefill_with_kv_cache_dtype_q_bf16_dtype_kv_bf16_dtype_o_bf16_dtype_idx_i32_head_dim_qk_128_head_dim_vo_128_posenc_0_use_swa_False_use_logits_cap_False_f16qk_False/batch_prefill_jit_pybind.cu(40): error: name followed by \"::\" must be a class or namespace name\n",
      "      int64_t window_left, bool enable_pdl , Optional<ffi::Tensor> maybe_custom_mask, Optional<ffi::Tensor> maybe_mask_indptr, Optional<ffi::Tensor> maybe_alibi_slopes, Optional<ffi::Tensor> maybe_prefix_len_ptr, Optional<ffi::Tensor> maybe_token_pos_in_items_ptr, Optional<ffi::Tensor> maybe_max_item_len_ptr, double logits_soft_cap, double sm_scale, double rope_rcp_scale, double rope_rcp_theta, int64_t token_pos_in_items_len);\n",
      "                                                                                                                                                                                  ^\n",
      "\n",
      "/root/.cache/flashinfer/80/generated/batch_prefill_with_kv_cache_dtype_q_bf16_dtype_kv_bf16_dtype_o_bf16_dtype_idx_i32_head_dim_qk_128_head_dim_vo_128_posenc_0_use_swa_False_use_logits_cap_False_f16qk_False/batch_prefill_jit_pybind.cu(40): error: Optional is not a template\n",
      "      int64_t window_left, bool enable_pdl , Optional<ffi::Tensor> maybe_custom_mask, Optional<ffi::Tensor> maybe_mask_indptr, Optional<ffi::Tensor> maybe_alibi_slopes, Optional<ffi::Tensor> maybe_prefix_len_ptr, Optional<ffi::Tensor> maybe_token_pos_in_items_ptr, Optional<ffi::Tensor> maybe_max_item_len_ptr, double logits_soft_cap, double sm_scale, double rope_rcp_scale, double rope_rcp_theta, int64_t token_pos_in_items_len);\n",
      "                                                                                                                                                                                                                     ^\n",
      "\n",
      "/root/.cache/flashinfer/80/generated/batch_prefill_with_kv_cache_dtype_q_bf16_dtype_kv_bf16_dtype_o_bf16_dtype_idx_i32_head_dim_qk_128_head_dim_vo_128_posenc_0_use_swa_False_use_logits_cap_False_f16qk_False/batch_prefill_jit_pybind.cu(40): error: name followed by \"::\" must be a class or namespace name\n",
      "      int64_t window_left, bool enable_pdl , Optional<ffi::Tensor> maybe_custom_mask, Optional<ffi::Tensor> maybe_mask_indptr, Optional<ffi::Tensor> maybe_alibi_slopes, Optional<ffi::Tensor> maybe_prefix_len_ptr, Optional<ffi::Tensor> maybe_token_pos_in_items_ptr, Optional<ffi::Tensor> maybe_max_item_len_ptr, double logits_soft_cap, double sm_scale, double rope_rcp_scale, double rope_rcp_theta, int64_t token_pos_in_items_len);\n",
      "                                                                                                                                                                                                                              ^\n",
      "\n",
      "/root/.cache/flashinfer/80/generated/batch_prefill_with_kv_cache_dtype_q_bf16_dtype_kv_bf16_dtype_o_bf16_dtype_idx_i32_head_dim_qk_128_head_dim_vo_128_posenc_0_use_swa_False_use_logits_cap_False_f16qk_False/batch_prefill_jit_pybind.cu(40): error: Optional is not a template\n",
      "      int64_t window_left, bool enable_pdl , Optional<ffi::Tensor> maybe_custom_mask, Optional<ffi::Tensor> maybe_mask_indptr, Optional<ffi::Tensor> maybe_alibi_slopes, Optional<ffi::Tensor> maybe_prefix_len_ptr, Optional<ffi::Tensor> maybe_token_pos_in_items_ptr, Optional<ffi::Tensor> maybe_max_item_len_ptr, double logits_soft_cap, double sm_scale, double rope_rcp_scale, double rope_rcp_theta, int64_t token_pos_in_items_len);\n",
      "                                                                                                                                                                                                                                                                         ^\n",
      "\n",
      "/root/.cache/flashinfer/80/generated/batch_prefill_with_kv_cache_dtype_q_bf16_dtype_kv_bf16_dtype_o_bf16_dtype_idx_i32_head_dim_qk_128_head_dim_vo_128_posenc_0_use_swa_False_use_logits_cap_False_f16qk_False/batch_prefill_jit_pybind.cu(40): error: name followed by \"::\" must be a class or namespace name\n",
      "      int64_t window_left, bool enable_pdl , Optional<ffi::Tensor> maybe_custom_mask, Optional<ffi::Tensor> maybe_mask_indptr, Optional<ffi::Tensor> maybe_alibi_slopes, Optional<ffi::Tensor> maybe_prefix_len_ptr, Optional<ffi::Tensor> maybe_token_pos_in_items_ptr, Optional<ffi::Tensor> maybe_max_item_len_ptr, double logits_soft_cap, double sm_scale, double rope_rcp_scale, double rope_rcp_theta, int64_t token_pos_in_items_len);\n",
      "                                                                                                                                                                                                                                                                                  ^\n",
      "\n",
      "24 errors detected in the compilation of \"/root/.cache/flashinfer/80/generated/batch_prefill_with_kv_cache_dtype_q_bf16_dtype_kv_bf16_dtype_o_bf16_dtype_idx_i32_head_dim_qk_128_head_dim_vo_128_posenc_0_use_swa_False_use_logits_cap_False_f16qk_False/batch_prefill_jit_pybind.cu\".\n",
      "ninja: build stopped: subcommand failed.\n",
      "\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/root/anaconda3/envs/PRewrite/lib/python3.12/site-packages/sglang/srt/managers/scheduler.py\", line 3034, in run_scheduler_process\n",
      "    scheduler = Scheduler(\n",
      "                ^^^^^^^^^^\n",
      "  File \"/root/anaconda3/envs/PRewrite/lib/python3.12/site-packages/sglang/srt/managers/scheduler.py\", line 418, in __init__\n",
      "    self.tp_worker = TpModelWorker(\n",
      "                     ^^^^^^^^^^^^^^\n",
      "  File \"/root/anaconda3/envs/PRewrite/lib/python3.12/site-packages/sglang/srt/managers/tp_worker.py\", line 95, in __init__\n",
      "    self.model_runner = ModelRunner(\n",
      "                        ^^^^^^^^^^^^\n",
      "  File \"/root/anaconda3/envs/PRewrite/lib/python3.12/site-packages/sglang/srt/model_executor/model_runner.py\", line 320, in __init__\n",
      "    self.initialize(min_per_gpu_memory)\n",
      "  File \"/root/anaconda3/envs/PRewrite/lib/python3.12/site-packages/sglang/srt/model_executor/model_runner.py\", line 468, in initialize\n",
      "    self.init_device_graphs()\n",
      "  File \"/root/anaconda3/envs/PRewrite/lib/python3.12/site-packages/sglang/srt/model_executor/model_runner.py\", line 2004, in init_device_graphs\n",
      "    self.graph_runner = graph_runners[self.device](self)\n",
      "                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/root/anaconda3/envs/PRewrite/lib/python3.12/site-packages/sglang/srt/model_executor/cuda_graph_runner.py\", line 368, in __init__\n",
      "    raise Exception(\n",
      "Exception: Capture cuda graph failed: Ninja build failed. Ninja output:\n",
      "ninja: Entering directory `/root/.cache/flashinfer/80/cached_ops'\n",
      "[1/3] /usr/local/cuda/bin/nvcc --generate-dependencies-with-compile --dependency-output batch_prefill_with_kv_cache_dtype_q_bf16_dtype_kv_bf16_dtype_o_bf16_dtype_idx_i32_head_dim_qk_128_head_dim_vo_128_posenc_0_use_swa_False_use_logits_cap_False_f16qk_False/batch_prefill.cuda.o.d -DTORCH_EXTENSION_NAME=batch_prefill_with_kv_cache_dtype_q_bf16_dtype_kv_bf16_dtype_o_bf16_dtype_idx_i32_head_dim_qk_128_head_dim_vo_128_posenc_0_use_swa_False_use_logits_cap_False_f16qk_False -DTORCH_API_INCLUDE_EXTENSION_H -DPy_LIMITED_API=0x03090000 -DPYBIND11_COMPILER_TYPE=\\\"_gcc\\\" -DPYBIND11_STDLIB=\\\"_libstdcpp\\\" -DPYBIND11_BUILD_ABI=\\\"_cxxabi1018\\\" -D_GLIBCXX_USE_CXX11_ABI=1 -isystem /root/anaconda3/envs/llm-serve/include/python3.12 -isystem /root/anaconda3/envs/llm-serve/lib/python3.12/site-packages/torch/include -isystem /root/anaconda3/envs/llm-serve/lib/python3.12/site-packages/torch/include/torch/csrc/api/include -isystem /usr/local/cuda/include -isystem /usr/local/cuda/include/cccl -isystem /root/anaconda3/envs/llm-serve/lib/python3.12/site-packages/flashinfer/data/include -isystem /root/anaconda3/envs/llm-serve/lib/python3.12/site-packages/flashinfer/data/csrc -isystem /root/anaconda3/envs/llm-serve/lib/python3.12/site-packages/flashinfer/data/cutlass/include -isystem /root/anaconda3/envs/llm-serve/lib/python3.12/site-packages/flashinfer/data/cutlass/tools/util/include -isystem /root/anaconda3/envs/llm-serve/lib/python3.12/site-packages/flashinfer/data/spdlog/include --compiler-options=-fPIC --expt-relaxed-constexpr -gencode=arch=compute_80,code=sm_80 -DFLASHINFER_ENABLE_FP8_E8M0 -DFLASHINFER_ENABLE_FP4_E2M1 -O3 -std=c++17 --threads=1 -use_fast_math -DFLASHINFER_ENABLE_F16 -DFLASHINFER_ENABLE_BF16 -DFLASHINFER_ENABLE_FP8_E4M3 -DFLASHINFER_ENABLE_FP8_E5M2 -DNDEBUG -lineinfo -c /root/.cache/flashinfer/80/generated/batch_prefill_with_kv_cache_dtype_q_bf16_dtype_kv_bf16_dtype_o_bf16_dtype_idx_i32_head_dim_qk_128_head_dim_vo_128_posenc_0_use_swa_False_use_logits_cap_False_f16qk_False/batch_prefill.cu -o batch_prefill_with_kv_cache_dtype_q_bf16_dtype_kv_bf16_dtype_o_bf16_dtype_idx_i32_head_dim_qk_128_head_dim_vo_128_posenc_0_use_swa_False_use_logits_cap_False_f16qk_False/batch_prefill.cuda.o \n",
      "\u001b[31mFAILED: [code=1] \u001b[0mbatch_prefill_with_kv_cache_dtype_q_bf16_dtype_kv_bf16_dtype_o_bf16_dtype_idx_i32_head_dim_qk_128_head_dim_vo_128_posenc_0_use_swa_False_use_logits_cap_False_f16qk_False/batch_prefill.cuda.o \n",
      "/usr/local/cuda/bin/nvcc --generate-dependencies-with-compile --dependency-output batch_prefill_with_kv_cache_dtype_q_bf16_dtype_kv_bf16_dtype_o_bf16_dtype_idx_i32_head_dim_qk_128_head_dim_vo_128_posenc_0_use_swa_False_use_logits_cap_False_f16qk_False/batch_prefill.cuda.o.d -DTORCH_EXTENSION_NAME=batch_prefill_with_kv_cache_dtype_q_bf16_dtype_kv_bf16_dtype_o_bf16_dtype_idx_i32_head_dim_qk_128_head_dim_vo_128_posenc_0_use_swa_False_use_logits_cap_False_f16qk_False -DTORCH_API_INCLUDE_EXTENSION_H -DPy_LIMITED_API=0x03090000 -DPYBIND11_COMPILER_TYPE=\\\"_gcc\\\" -DPYBIND11_STDLIB=\\\"_libstdcpp\\\" -DPYBIND11_BUILD_ABI=\\\"_cxxabi1018\\\" -D_GLIBCXX_USE_CXX11_ABI=1 -isystem /root/anaconda3/envs/llm-serve/include/python3.12 -isystem /root/anaconda3/envs/llm-serve/lib/python3.12/site-packages/torch/include -isystem /root/anaconda3/envs/llm-serve/lib/python3.12/site-packages/torch/include/torch/csrc/api/include -isystem /usr/local/cuda/include -isystem /usr/local/cuda/include/cccl -isystem /root/anaconda3/envs/llm-serve/lib/python3.12/site-packages/flashinfer/data/include -isystem /root/anaconda3/envs/llm-serve/lib/python3.12/site-packages/flashinfer/data/csrc -isystem /root/anaconda3/envs/llm-serve/lib/python3.12/site-packages/flashinfer/data/cutlass/include -isystem /root/anaconda3/envs/llm-serve/lib/python3.12/site-packages/flashinfer/data/cutlass/tools/util/include -isystem /root/anaconda3/envs/llm-serve/lib/python3.12/site-packages/flashinfer/data/spdlog/include --compiler-options=-fPIC --expt-relaxed-constexpr -gencode=arch=compute_80,code=sm_80 -DFLASHINFER_ENABLE_FP8_E8M0 -DFLASHINFER_ENABLE_FP4_E2M1 -O3 -std=c++17 --threads=1 -use_fast_math -DFLASHINFER_ENABLE_F16 -DFLASHINFER_ENABLE_BF16 -DFLASHINFER_ENABLE_FP8_E4M3 -DFLASHINFER_ENABLE_FP8_E5M2 -DNDEBUG -lineinfo -c /root/.cache/flashinfer/80/generated/batch_prefill_with_kv_cache_dtype_q_bf16_dtype_kv_bf16_dtype_o_bf16_dtype_idx_i32_head_dim_qk_128_head_dim_vo_128_posenc_0_use_swa_False_use_logits_cap_False_f16qk_False/batch_prefill.cu -o batch_prefill_with_kv_cache_dtype_q_bf16_dtype_kv_bf16_dtype_o_bf16_dtype_idx_i32_head_dim_qk_128_head_dim_vo_128_posenc_0_use_swa_False_use_logits_cap_False_f16qk_False/batch_prefill.cuda.o \n",
      "/root/.cache/flashinfer/80/generated/batch_prefill_with_kv_cache_dtype_q_bf16_dtype_kv_bf16_dtype_o_bf16_dtype_idx_i32_head_dim_qk_128_head_dim_vo_128_posenc_0_use_swa_False_use_logits_cap_False_f16qk_False/batch_prefill.cu:21:10: fatal error: tvm/ffi/container/array.h: No such file or directory\n",
      "   21 | #include \"tvm/ffi/container/array.h\"\n",
      "      |          ^~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "compilation terminated.\n",
      "[2/3] /usr/local/cuda/bin/nvcc --generate-dependencies-with-compile --dependency-output batch_prefill_with_kv_cache_dtype_q_bf16_dtype_kv_bf16_dtype_o_bf16_dtype_idx_i32_head_dim_qk_128_head_dim_vo_128_posenc_0_use_swa_False_use_logits_cap_False_f16qk_False/batch_prefill_jit_pybind.cuda.o.d -DTORCH_EXTENSION_NAME=batch_prefill_with_kv_cache_dtype_q_bf16_dtype_kv_bf16_dtype_o_bf16_dtype_idx_i32_head_dim_qk_128_head_dim_vo_128_posenc_0_use_swa_False_use_logits_cap_False_f16qk_False -DTORCH_API_INCLUDE_EXTENSION_H -DPy_LIMITED_API=0x03090000 -DPYBIND11_COMPILER_TYPE=\\\"_gcc\\\" -DPYBIND11_STDLIB=\\\"_libstdcpp\\\" -DPYBIND11_BUILD_ABI=\\\"_cxxabi1018\\\" -D_GLIBCXX_USE_CXX11_ABI=1 -isystem /root/anaconda3/envs/llm-serve/include/python3.12 -isystem /root/anaconda3/envs/llm-serve/lib/python3.12/site-packages/torch/include -isystem /root/anaconda3/envs/llm-serve/lib/python3.12/site-packages/torch/include/torch/csrc/api/include -isystem /usr/local/cuda/include -isystem /usr/local/cuda/include/cccl -isystem /root/anaconda3/envs/llm-serve/lib/python3.12/site-packages/flashinfer/data/include -isystem /root/anaconda3/envs/llm-serve/lib/python3.12/site-packages/flashinfer/data/csrc -isystem /root/anaconda3/envs/llm-serve/lib/python3.12/site-packages/flashinfer/data/cutlass/include -isystem /root/anaconda3/envs/llm-serve/lib/python3.12/site-packages/flashinfer/data/cutlass/tools/util/include -isystem /root/anaconda3/envs/llm-serve/lib/python3.12/site-packages/flashinfer/data/spdlog/include --compiler-options=-fPIC --expt-relaxed-constexpr -gencode=arch=compute_80,code=sm_80 -DFLASHINFER_ENABLE_FP8_E8M0 -DFLASHINFER_ENABLE_FP4_E2M1 -O3 -std=c++17 --threads=1 -use_fast_math -DFLASHINFER_ENABLE_F16 -DFLASHINFER_ENABLE_BF16 -DFLASHINFER_ENABLE_FP8_E4M3 -DFLASHINFER_ENABLE_FP8_E5M2 -DNDEBUG -lineinfo -c /root/.cache/flashinfer/80/generated/batch_prefill_with_kv_cache_dtype_q_bf16_dtype_kv_bf16_dtype_o_bf16_dtype_idx_i32_head_dim_qk_128_head_dim_vo_128_posenc_0_use_swa_False_use_logits_cap_False_f16qk_False/batch_prefill_jit_pybind.cu -o batch_prefill_with_kv_cache_dtype_q_bf16_dtype_kv_bf16_dtype_o_bf16_dtype_idx_i32_head_dim_qk_128_head_dim_vo_128_posenc_0_use_swa_False_use_logits_cap_False_f16qk_False/batch_prefill_jit_pybind.cuda.o \n",
      "\u001b[31mFAILED: [code=2] \u001b[0mbatch_prefill_with_kv_cache_dtype_q_bf16_dtype_kv_bf16_dtype_o_bf16_dtype_idx_i32_head_dim_qk_128_head_dim_vo_128_posenc_0_use_swa_False_use_logits_cap_False_f16qk_False/batch_prefill_jit_pybind.cuda.o \n",
      "/usr/local/cuda/bin/nvcc --generate-dependencies-with-compile --dependency-output batch_prefill_with_kv_cache_dtype_q_bf16_dtype_kv_bf16_dtype_o_bf16_dtype_idx_i32_head_dim_qk_128_head_dim_vo_128_posenc_0_use_swa_False_use_logits_cap_False_f16qk_False/batch_prefill_jit_pybind.cuda.o.d -DTORCH_EXTENSION_NAME=batch_prefill_with_kv_cache_dtype_q_bf16_dtype_kv_bf16_dtype_o_bf16_dtype_idx_i32_head_dim_qk_128_head_dim_vo_128_posenc_0_use_swa_False_use_logits_cap_False_f16qk_False -DTORCH_API_INCLUDE_EXTENSION_H -DPy_LIMITED_API=0x03090000 -DPYBIND11_COMPILER_TYPE=\\\"_gcc\\\" -DPYBIND11_STDLIB=\\\"_libstdcpp\\\" -DPYBIND11_BUILD_ABI=\\\"_cxxabi1018\\\" -D_GLIBCXX_USE_CXX11_ABI=1 -isystem /root/anaconda3/envs/llm-serve/include/python3.12 -isystem /root/anaconda3/envs/llm-serve/lib/python3.12/site-packages/torch/include -isystem /root/anaconda3/envs/llm-serve/lib/python3.12/site-packages/torch/include/torch/csrc/api/include -isystem /usr/local/cuda/include -isystem /usr/local/cuda/include/cccl -isystem /root/anaconda3/envs/llm-serve/lib/python3.12/site-packages/flashinfer/data/include -isystem /root/anaconda3/envs/llm-serve/lib/python3.12/site-packages/flashinfer/data/csrc -isystem /root/anaconda3/envs/llm-serve/lib/python3.12/site-packages/flashinfer/data/cutlass/include -isystem /root/anaconda3/envs/llm-serve/lib/python3.12/site-packages/flashinfer/data/cutlass/tools/util/include -isystem /root/anaconda3/envs/llm-serve/lib/python3.12/site-packages/flashinfer/data/spdlog/include --compiler-options=-fPIC --expt-relaxed-constexpr -gencode=arch=compute_80,code=sm_80 -DFLASHINFER_ENABLE_FP8_E8M0 -DFLASHINFER_ENABLE_FP4_E2M1 -O3 -std=c++17 --threads=1 -use_fast_math -DFLASHINFER_ENABLE_F16 -DFLASHINFER_ENABLE_BF16 -DFLASHINFER_ENABLE_FP8_E4M3 -DFLASHINFER_ENABLE_FP8_E5M2 -DNDEBUG -lineinfo -c /root/.cache/flashinfer/80/generated/batch_prefill_with_kv_cache_dtype_q_bf16_dtype_kv_bf16_dtype_o_bf16_dtype_idx_i32_head_dim_qk_128_head_dim_vo_128_posenc_0_use_swa_False_use_logits_cap_False_f16qk_False/batch_prefill_jit_pybind.cu -o batch_prefill_with_kv_cache_dtype_q_bf16_dtype_kv_bf16_dtype_o_bf16_dtype_idx_i32_head_dim_qk_128_head_dim_vo_128_posenc_0_use_swa_False_use_logits_cap_False_f16qk_False/batch_prefill_jit_pybind.cuda.o \n",
      "/root/.cache/flashinfer/80/generated/batch_prefill_with_kv_cache_dtype_q_bf16_dtype_kv_bf16_dtype_o_bf16_dtype_idx_i32_head_dim_qk_128_head_dim_vo_128_posenc_0_use_swa_False_use_logits_cap_False_f16qk_False/batch_prefill_jit_pybind.cu(33): error: Optional is not a template\n",
      "                                        bool enable_pdl , Optional<ffi::Tensor> maybe_custom_mask, Optional<ffi::Tensor> maybe_mask_indptr, Optional<ffi::Tensor> maybe_alibi_slopes, Optional<ffi::Tensor> maybe_prefix_len_ptr, Optional<ffi::Tensor> maybe_token_pos_in_items_ptr, Optional<ffi::Tensor> maybe_max_item_len_ptr, double logits_soft_cap, double sm_scale, double rope_rcp_scale, double rope_rcp_theta, int64_t token_pos_in_items_len);\n",
      "                                                          ^\n",
      "\n",
      "/root/.cache/flashinfer/80/generated/batch_prefill_with_kv_cache_dtype_q_bf16_dtype_kv_bf16_dtype_o_bf16_dtype_idx_i32_head_dim_qk_128_head_dim_vo_128_posenc_0_use_swa_False_use_logits_cap_False_f16qk_False/batch_prefill_jit_pybind.cu(33): error: name followed by \"::\" must be a class or namespace name\n",
      "                                        bool enable_pdl , Optional<ffi::Tensor> maybe_custom_mask, Optional<ffi::Tensor> maybe_mask_indptr, Optional<ffi::Tensor> maybe_alibi_slopes, Optional<ffi::Tensor> maybe_prefix_len_ptr, Optional<ffi::Tensor> maybe_token_pos_in_items_ptr, Optional<ffi::Tensor> maybe_max_item_len_ptr, double logits_soft_cap, double sm_scale, double rope_rcp_scale, double rope_rcp_theta, int64_t token_pos_in_items_len);\n",
      "                                                                   ^\n",
      "\n",
      "/root/.cache/flashinfer/80/generated/batch_prefill_with_kv_cache_dtype_q_bf16_dtype_kv_bf16_dtype_o_bf16_dtype_idx_i32_head_dim_qk_128_head_dim_vo_128_posenc_0_use_swa_False_use_logits_cap_False_f16qk_False/batch_prefill_jit_pybind.cu(33): error: Optional is not a template\n",
      "                                        bool enable_pdl , Optional<ffi::Tensor> maybe_custom_mask, Optional<ffi::Tensor> maybe_mask_indptr, Optional<ffi::Tensor> maybe_alibi_slopes, Optional<ffi::Tensor> maybe_prefix_len_ptr, Optional<ffi::Tensor> maybe_token_pos_in_items_ptr, Optional<ffi::Tensor> maybe_max_item_len_ptr, double logits_soft_cap, double sm_scale, double rope_rcp_scale, double rope_rcp_theta, int64_t token_pos_in_items_len);\n",
      "                                                                                                   ^\n",
      "\n",
      "/root/.cache/flashinfer/80/generated/batch_prefill_with_kv_cache_dtype_q_bf16_dtype_kv_bf16_dtype_o_bf16_dtype_idx_i32_head_dim_qk_128_head_dim_vo_128_posenc_0_use_swa_False_use_logits_cap_False_f16qk_False/batch_prefill_jit_pybind.cu(33): error: name followed by \"::\" must be a class or namespace name\n",
      "                                        bool enable_pdl , Optional<ffi::Tensor> maybe_custom_mask, Optional<ffi::Tensor> maybe_mask_indptr, Optional<ffi::Tensor> maybe_alibi_slopes, Optional<ffi::Tensor> maybe_prefix_len_ptr, Optional<ffi::Tensor> maybe_token_pos_in_items_ptr, Optional<ffi::Tensor> maybe_max_item_len_ptr, double logits_soft_cap, double sm_scale, double rope_rcp_scale, double rope_rcp_theta, int64_t token_pos_in_items_len);\n",
      "                                                                                                            ^\n",
      "\n",
      "/root/.cache/flashinfer/80/generated/batch_prefill_with_kv_cache_dtype_q_bf16_dtype_kv_bf16_dtype_o_bf16_dtype_idx_i32_head_dim_qk_128_head_dim_vo_128_posenc_0_use_swa_False_use_logits_cap_False_f16qk_False/batch_prefill_jit_pybind.cu(33): error: Optional is not a template\n",
      "                                        bool enable_pdl , Optional<ffi::Tensor> maybe_custom_mask, Optional<ffi::Tensor> maybe_mask_indptr, Optional<ffi::Tensor> maybe_alibi_slopes, Optional<ffi::Tensor> maybe_prefix_len_ptr, Optional<ffi::Tensor> maybe_token_pos_in_items_ptr, Optional<ffi::Tensor> maybe_max_item_len_ptr, double logits_soft_cap, double sm_scale, double rope_rcp_scale, double rope_rcp_theta, int64_t token_pos_in_items_len);\n",
      "                                                                                                                                            ^\n",
      "\n",
      "/root/.cache/flashinfer/80/generated/batch_prefill_with_kv_cache_dtype_q_bf16_dtype_kv_bf16_dtype_o_bf16_dtype_idx_i32_head_dim_qk_128_head_dim_vo_128_posenc_0_use_swa_False_use_logits_cap_False_f16qk_False/batch_prefill_jit_pybind.cu(33): error: name followed by \"::\" must be a class or namespace name\n",
      "                                        bool enable_pdl , Optional<ffi::Tensor> maybe_custom_mask, Optional<ffi::Tensor> maybe_mask_indptr, Optional<ffi::Tensor> maybe_alibi_slopes, Optional<ffi::Tensor> maybe_prefix_len_ptr, Optional<ffi::Tensor> maybe_token_pos_in_items_ptr, Optional<ffi::Tensor> maybe_max_item_len_ptr, double logits_soft_cap, double sm_scale, double rope_rcp_scale, double rope_rcp_theta, int64_t token_pos_in_items_len);\n",
      "                                                                                                                                                     ^\n",
      "\n",
      "/root/.cache/flashinfer/80/generated/batch_prefill_with_kv_cache_dtype_q_bf16_dtype_kv_bf16_dtype_o_bf16_dtype_idx_i32_head_dim_qk_128_head_dim_vo_128_posenc_0_use_swa_False_use_logits_cap_False_f16qk_False/batch_prefill_jit_pybind.cu(33): error: Optional is not a template\n",
      "                                        bool enable_pdl , Optional<ffi::Tensor> maybe_custom_mask, Optional<ffi::Tensor> maybe_mask_indptr, Optional<ffi::Tensor> maybe_alibi_slopes, Optional<ffi::Tensor> maybe_prefix_len_ptr, Optional<ffi::Tensor> maybe_token_pos_in_items_ptr, Optional<ffi::Tensor> maybe_max_item_len_ptr, double logits_soft_cap, double sm_scale, double rope_rcp_scale, double rope_rcp_theta, int64_t token_pos_in_items_len);\n",
      "                                                                                                                                                                                      ^\n",
      "\n",
      "/root/.cache/flashinfer/80/generated/batch_prefill_with_kv_cache_dtype_q_bf16_dtype_kv_bf16_dtype_o_bf16_dtype_idx_i32_head_dim_qk_128_head_dim_vo_128_posenc_0_use_swa_False_use_logits_cap_False_f16qk_False/batch_prefill_jit_pybind.cu(33): error: name followed by \"::\" must be a class or namespace name\n",
      "                                        bool enable_pdl , Optional<ffi::Tensor> maybe_custom_mask, Optional<ffi::Tensor> maybe_mask_indptr, Optional<ffi::Tensor> maybe_alibi_slopes, Optional<ffi::Tensor> maybe_prefix_len_ptr, Optional<ffi::Tensor> maybe_token_pos_in_items_ptr, Optional<ffi::Tensor> maybe_max_item_len_ptr, double logits_soft_cap, double sm_scale, double rope_rcp_scale, double rope_rcp_theta, int64_t token_pos_in_items_len);\n",
      "                                                                                                                                                                                               ^\n",
      "\n",
      "/root/.cache/flashinfer/80/generated/batch_prefill_with_kv_cache_dtype_q_bf16_dtype_kv_bf16_dtype_o_bf16_dtype_idx_i32_head_dim_qk_128_head_dim_vo_128_posenc_0_use_swa_False_use_logits_cap_False_f16qk_False/batch_prefill_jit_pybind.cu(33): error: Optional is not a template\n",
      "                                        bool enable_pdl , Optional<ffi::Tensor> maybe_custom_mask, Optional<ffi::Tensor> maybe_mask_indptr, Optional<ffi::Tensor> maybe_alibi_slopes, Optional<ffi::Tensor> maybe_prefix_len_ptr, Optional<ffi::Tensor> maybe_token_pos_in_items_ptr, Optional<ffi::Tensor> maybe_max_item_len_ptr, double logits_soft_cap, double sm_scale, double rope_rcp_scale, double rope_rcp_theta, int64_t token_pos_in_items_len);\n",
      "                                                                                                                                                                                                                                  ^\n",
      "\n",
      "/root/.cache/flashinfer/80/generated/batch_prefill_with_kv_cache_dtype_q_bf16_dtype_kv_bf16_dtype_o_bf16_dtype_idx_i32_head_dim_qk_128_head_dim_vo_128_posenc_0_use_swa_False_use_logits_cap_False_f16qk_False/batch_prefill_jit_pybind.cu(33): error: name followed by \"::\" must be a class or namespace name\n",
      "                                        bool enable_pdl , Optional<ffi::Tensor> maybe_custom_mask, Optional<ffi::Tensor> maybe_mask_indptr, Optional<ffi::Tensor> maybe_alibi_slopes, Optional<ffi::Tensor> maybe_prefix_len_ptr, Optional<ffi::Tensor> maybe_token_pos_in_items_ptr, Optional<ffi::Tensor> maybe_max_item_len_ptr, double logits_soft_cap, double sm_scale, double rope_rcp_scale, double rope_rcp_theta, int64_t token_pos_in_items_len);\n",
      "                                                                                                                                                                                                                                           ^\n",
      "\n",
      "/root/.cache/flashinfer/80/generated/batch_prefill_with_kv_cache_dtype_q_bf16_dtype_kv_bf16_dtype_o_bf16_dtype_idx_i32_head_dim_qk_128_head_dim_vo_128_posenc_0_use_swa_False_use_logits_cap_False_f16qk_False/batch_prefill_jit_pybind.cu(33): error: Optional is not a template\n",
      "                                        bool enable_pdl , Optional<ffi::Tensor> maybe_custom_mask, Optional<ffi::Tensor> maybe_mask_indptr, Optional<ffi::Tensor> maybe_alibi_slopes, Optional<ffi::Tensor> maybe_prefix_len_ptr, Optional<ffi::Tensor> maybe_token_pos_in_items_ptr, Optional<ffi::Tensor> maybe_max_item_len_ptr, double logits_soft_cap, double sm_scale, double rope_rcp_scale, double rope_rcp_theta, int64_t token_pos_in_items_len);\n",
      "                                                                                                                                                                                                                                                                                      ^\n",
      "\n",
      "/root/.cache/flashinfer/80/generated/batch_prefill_with_kv_cache_dtype_q_bf16_dtype_kv_bf16_dtype_o_bf16_dtype_idx_i32_head_dim_qk_128_head_dim_vo_128_posenc_0_use_swa_False_use_logits_cap_False_f16qk_False/batch_prefill_jit_pybind.cu(33): error: name followed by \"::\" must be a class or namespace name\n",
      "                                        bool enable_pdl , Optional<ffi::Tensor> maybe_custom_mask, Optional<ffi::Tensor> maybe_mask_indptr, Optional<ffi::Tensor> maybe_alibi_slopes, Optional<ffi::Tensor> maybe_prefix_len_ptr, Optional<ffi::Tensor> maybe_token_pos_in_items_ptr, Optional<ffi::Tensor> maybe_max_item_len_ptr, double logits_soft_cap, double sm_scale, double rope_rcp_scale, double rope_rcp_theta, int64_t token_pos_in_items_len);\n",
      "                                                                                                                                                                                                                                                                                               ^\n",
      "\n",
      "/root/.cache/flashinfer/80/generated/batch_prefill_with_kv_cache_dtype_q_bf16_dtype_kv_bf16_dtype_o_bf16_dtype_idx_i32_head_dim_qk_128_head_dim_vo_128_posenc_0_use_swa_False_use_logits_cap_False_f16qk_False/batch_prefill_jit_pybind.cu(40): error: Optional is not a template\n",
      "      int64_t window_left, bool enable_pdl , Optional<ffi::Tensor> maybe_custom_mask, Optional<ffi::Tensor> maybe_mask_indptr, Optional<ffi::Tensor> maybe_alibi_slopes, Optional<ffi::Tensor> maybe_prefix_len_ptr, Optional<ffi::Tensor> maybe_token_pos_in_items_ptr, Optional<ffi::Tensor> maybe_max_item_len_ptr, double logits_soft_cap, double sm_scale, double rope_rcp_scale, double rope_rcp_theta, int64_t token_pos_in_items_len);\n",
      "                                             ^\n",
      "\n",
      "/root/.cache/flashinfer/80/generated/batch_prefill_with_kv_cache_dtype_q_bf16_dtype_kv_bf16_dtype_o_bf16_dtype_idx_i32_head_dim_qk_128_head_dim_vo_128_posenc_0_use_swa_False_use_logits_cap_False_f16qk_False/batch_prefill_jit_pybind.cu(40): error: name followed by \"::\" must be a class or namespace name\n",
      "      int64_t window_left, bool enable_pdl , Optional<ffi::Tensor> maybe_custom_mask, Optional<ffi::Tensor> maybe_mask_indptr, Optional<ffi::Tensor> maybe_alibi_slopes, Optional<ffi::Tensor> maybe_prefix_len_ptr, Optional<ffi::Tensor> maybe_token_pos_in_items_ptr, Optional<ffi::Tensor> maybe_max_item_len_ptr, double logits_soft_cap, double sm_scale, double rope_rcp_scale, double rope_rcp_theta, int64_t token_pos_in_items_len);\n",
      "                                                      ^\n",
      "\n",
      "/root/.cache/flashinfer/80/generated/batch_prefill_with_kv_cache_dtype_q_bf16_dtype_kv_bf16_dtype_o_bf16_dtype_idx_i32_head_dim_qk_128_head_dim_vo_128_posenc_0_use_swa_False_use_logits_cap_False_f16qk_False/batch_prefill_jit_pybind.cu(40): error: Optional is not a template\n",
      "      int64_t window_left, bool enable_pdl , Optional<ffi::Tensor> maybe_custom_mask, Optional<ffi::Tensor> maybe_mask_indptr, Optional<ffi::Tensor> maybe_alibi_slopes, Optional<ffi::Tensor> maybe_prefix_len_ptr, Optional<ffi::Tensor> maybe_token_pos_in_items_ptr, Optional<ffi::Tensor> maybe_max_item_len_ptr, double logits_soft_cap, double sm_scale, double rope_rcp_scale, double rope_rcp_theta, int64_t token_pos_in_items_len);\n",
      "                                                                                      ^\n",
      "\n",
      "/root/.cache/flashinfer/80/generated/batch_prefill_with_kv_cache_dtype_q_bf16_dtype_kv_bf16_dtype_o_bf16_dtype_idx_i32_head_dim_qk_128_head_dim_vo_128_posenc_0_use_swa_False_use_logits_cap_False_f16qk_False/batch_prefill_jit_pybind.cu(40): error: name followed by \"::\" must be a class or namespace name\n",
      "      int64_t window_left, bool enable_pdl , Optional<ffi::Tensor> maybe_custom_mask, Optional<ffi::Tensor> maybe_mask_indptr, Optional<ffi::Tensor> maybe_alibi_slopes, Optional<ffi::Tensor> maybe_prefix_len_ptr, Optional<ffi::Tensor> maybe_token_pos_in_items_ptr, Optional<ffi::Tensor> maybe_max_item_len_ptr, double logits_soft_cap, double sm_scale, double rope_rcp_scale, double rope_rcp_theta, int64_t token_pos_in_items_len);\n",
      "                                                                                               ^\n",
      "\n",
      "/root/.cache/flashinfer/80/generated/batch_prefill_with_kv_cache_dtype_q_bf16_dtype_kv_bf16_dtype_o_bf16_dtype_idx_i32_head_dim_qk_128_head_dim_vo_128_posenc_0_use_swa_False_use_logits_cap_False_f16qk_False/batch_prefill_jit_pybind.cu(40): error: Optional is not a template\n",
      "      int64_t window_left, bool enable_pdl , Optional<ffi::Tensor> maybe_custom_mask, Optional<ffi::Tensor> maybe_mask_indptr, Optional<ffi::Tensor> maybe_alibi_slopes, Optional<ffi::Tensor> maybe_prefix_len_ptr, Optional<ffi::Tensor> maybe_token_pos_in_items_ptr, Optional<ffi::Tensor> maybe_max_item_len_ptr, double logits_soft_cap, double sm_scale, double rope_rcp_scale, double rope_rcp_theta, int64_t token_pos_in_items_len);\n",
      "                                                                                                                               ^\n",
      "\n",
      "/root/.cache/flashinfer/80/generated/batch_prefill_with_kv_cache_dtype_q_bf16_dtype_kv_bf16_dtype_o_bf16_dtype_idx_i32_head_dim_qk_128_head_dim_vo_128_posenc_0_use_swa_False_use_logits_cap_False_f16qk_False/batch_prefill_jit_pybind.cu(40): error: name followed by \"::\" must be a class or namespace name\n",
      "      int64_t window_left, bool enable_pdl , Optional<ffi::Tensor> maybe_custom_mask, Optional<ffi::Tensor> maybe_mask_indptr, Optional<ffi::Tensor> maybe_alibi_slopes, Optional<ffi::Tensor> maybe_prefix_len_ptr, Optional<ffi::Tensor> maybe_token_pos_in_items_ptr, Optional<ffi::Tensor> maybe_max_item_len_ptr, double logits_soft_cap, double sm_scale, double rope_rcp_scale, double rope_rcp_theta, int64_t token_pos_in_items_len);\n",
      "                                                                                                                                        ^\n",
      "\n",
      "/root/.cache/flashinfer/80/generated/batch_prefill_with_kv_cache_dtype_q_bf16_dtype_kv_bf16_dtype_o_bf16_dtype_idx_i32_head_dim_qk_128_head_dim_vo_128_posenc_0_use_swa_False_use_logits_cap_False_f16qk_False/batch_prefill_jit_pybind.cu(40): error: Optional is not a template\n",
      "      int64_t window_left, bool enable_pdl , Optional<ffi::Tensor> maybe_custom_mask, Optional<ffi::Tensor> maybe_mask_indptr, Optional<ffi::Tensor> maybe_alibi_slopes, Optional<ffi::Tensor> maybe_prefix_len_ptr, Optional<ffi::Tensor> maybe_token_pos_in_items_ptr, Optional<ffi::Tensor> maybe_max_item_len_ptr, double logits_soft_cap, double sm_scale, double rope_rcp_scale, double rope_rcp_theta, int64_t token_pos_in_items_len);\n",
      "                                                                                                                                                                         ^\n",
      "\n",
      "/root/.cache/flashinfer/80/generated/batch_prefill_with_kv_cache_dtype_q_bf16_dtype_kv_bf16_dtype_o_bf16_dtype_idx_i32_head_dim_qk_128_head_dim_vo_128_posenc_0_use_swa_False_use_logits_cap_False_f16qk_False/batch_prefill_jit_pybind.cu(40): error: name followed by \"::\" must be a class or namespace name\n",
      "      int64_t window_left, bool enable_pdl , Optional<ffi::Tensor> maybe_custom_mask, Optional<ffi::Tensor> maybe_mask_indptr, Optional<ffi::Tensor> maybe_alibi_slopes, Optional<ffi::Tensor> maybe_prefix_len_ptr, Optional<ffi::Tensor> maybe_token_pos_in_items_ptr, Optional<ffi::Tensor> maybe_max_item_len_ptr, double logits_soft_cap, double sm_scale, double rope_rcp_scale, double rope_rcp_theta, int64_t token_pos_in_items_len);\n",
      "                                                                                                                                                                                  ^\n",
      "\n",
      "/root/.cache/flashinfer/80/generated/batch_prefill_with_kv_cache_dtype_q_bf16_dtype_kv_bf16_dtype_o_bf16_dtype_idx_i32_head_dim_qk_128_head_dim_vo_128_posenc_0_use_swa_False_use_logits_cap_False_f16qk_False/batch_prefill_jit_pybind.cu(40): error: Optional is not a template\n",
      "      int64_t window_left, bool enable_pdl , Optional<ffi::Tensor> maybe_custom_mask, Optional<ffi::Tensor> maybe_mask_indptr, Optional<ffi::Tensor> maybe_alibi_slopes, Optional<ffi::Tensor> maybe_prefix_len_ptr, Optional<ffi::Tensor> maybe_token_pos_in_items_ptr, Optional<ffi::Tensor> maybe_max_item_len_ptr, double logits_soft_cap, double sm_scale, double rope_rcp_scale, double rope_rcp_theta, int64_t token_pos_in_items_len);\n",
      "                                                                                                                                                                                                                     ^\n",
      "\n",
      "/root/.cache/flashinfer/80/generated/batch_prefill_with_kv_cache_dtype_q_bf16_dtype_kv_bf16_dtype_o_bf16_dtype_idx_i32_head_dim_qk_128_head_dim_vo_128_posenc_0_use_swa_False_use_logits_cap_False_f16qk_False/batch_prefill_jit_pybind.cu(40): error: name followed by \"::\" must be a class or namespace name\n",
      "      int64_t window_left, bool enable_pdl , Optional<ffi::Tensor> maybe_custom_mask, Optional<ffi::Tensor> maybe_mask_indptr, Optional<ffi::Tensor> maybe_alibi_slopes, Optional<ffi::Tensor> maybe_prefix_len_ptr, Optional<ffi::Tensor> maybe_token_pos_in_items_ptr, Optional<ffi::Tensor> maybe_max_item_len_ptr, double logits_soft_cap, double sm_scale, double rope_rcp_scale, double rope_rcp_theta, int64_t token_pos_in_items_len);\n",
      "                                                                                                                                                                                                                              ^\n",
      "\n",
      "/root/.cache/flashinfer/80/generated/batch_prefill_with_kv_cache_dtype_q_bf16_dtype_kv_bf16_dtype_o_bf16_dtype_idx_i32_head_dim_qk_128_head_dim_vo_128_posenc_0_use_swa_False_use_logits_cap_False_f16qk_False/batch_prefill_jit_pybind.cu(40): error: Optional is not a template\n",
      "      int64_t window_left, bool enable_pdl , Optional<ffi::Tensor> maybe_custom_mask, Optional<ffi::Tensor> maybe_mask_indptr, Optional<ffi::Tensor> maybe_alibi_slopes, Optional<ffi::Tensor> maybe_prefix_len_ptr, Optional<ffi::Tensor> maybe_token_pos_in_items_ptr, Optional<ffi::Tensor> maybe_max_item_len_ptr, double logits_soft_cap, double sm_scale, double rope_rcp_scale, double rope_rcp_theta, int64_t token_pos_in_items_len);\n",
      "                                                                                                                                                                                                                                                                         ^\n",
      "\n",
      "/root/.cache/flashinfer/80/generated/batch_prefill_with_kv_cache_dtype_q_bf16_dtype_kv_bf16_dtype_o_bf16_dtype_idx_i32_head_dim_qk_128_head_dim_vo_128_posenc_0_use_swa_False_use_logits_cap_False_f16qk_False/batch_prefill_jit_pybind.cu(40): error: name followed by \"::\" must be a class or namespace name\n",
      "      int64_t window_left, bool enable_pdl , Optional<ffi::Tensor> maybe_custom_mask, Optional<ffi::Tensor> maybe_mask_indptr, Optional<ffi::Tensor> maybe_alibi_slopes, Optional<ffi::Tensor> maybe_prefix_len_ptr, Optional<ffi::Tensor> maybe_token_pos_in_items_ptr, Optional<ffi::Tensor> maybe_max_item_len_ptr, double logits_soft_cap, double sm_scale, double rope_rcp_scale, double rope_rcp_theta, int64_t token_pos_in_items_len);\n",
      "                                                                                                                                                                                                                                                                                  ^\n",
      "\n",
      "24 errors detected in the compilation of \"/root/.cache/flashinfer/80/generated/batch_prefill_with_kv_cache_dtype_q_bf16_dtype_kv_bf16_dtype_o_bf16_dtype_idx_i32_head_dim_qk_128_head_dim_vo_128_posenc_0_use_swa_False_use_logits_cap_False_f16qk_False/batch_prefill_jit_pybind.cu\".\n",
      "ninja: build stopped: subcommand failed.\n",
      "\n",
      "Possible solutions:\n",
      "1. set --mem-fraction-static to a smaller value (e.g., 0.8 or 0.7)\n",
      "2. set --cuda-graph-max-bs to a smaller value (e.g., 16)\n",
      "3. disable torch compile by not using --enable-torch-compile\n",
      "4. disable CUDA graph by --disable-cuda-graph. (Not recommended. Huge performance loss)\n",
      "Open an issue on GitHub https://github.com/sgl-project/sglang/issues/new/choose \n",
      "\n",
      "\n",
      "[2025-11-03 02:36:08] Received sigquit from a child process. It usually means the child failed.\n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "generate_model = sgl.Engine(\n",
    "    model_path=\"/root/group-shared/jrc/base_models/Qwen3-32B\",\n",
    "    tp_size=8,\n",
    "    enable_custom_logit_processor=True,\n",
    "    enable_p2p_check=True,\n",
    "    mem_fraction_static=0.3,\n",
    "    max_prefill_tokens=131072,\n",
    "    log_level=\"info\",\n",
    "    trust_remote_code=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22d88d64",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PRewrite",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
